{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import random\n",
    "random.seed(1234)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import bsg, tools\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, FastText, KeyedVectors\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/math689env/lib/python3.7/site-packages/ipykernel/__main__.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "princeton_data = pd.read_csv(\"articles_1999.csv\", delimiter=\"\\|\\~\\|\", header=None)\n",
    "princeton_data.columns = [\"ID\", \"headline\", \"date\", \"type\", \"author\", \"author_title\", \"article\", \"author_bio\"]\n",
    "princeton_data = princeton_data[princeton_data[\"article\"].notnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prince_data = list(princeton_data.loc[0:40,\"article\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prince_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Stuff ###\n",
    "\n",
    "window = 2\n",
    "t = np.power(10.0,-4.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, tokenized_corpus, word2idx, idx2word = tools.tokenize_corpus(prince_data)\n",
    "\n",
    "indexed_corpus = tools.index_tokenized_corpus(tokenized_corpus, word2idx)\n",
    "\n",
    "flattened_indexed_corpus = tools.flatten(indexed_corpus)\n",
    "unigram_subsampling = tools.make_unigram_dict(flattened_indexed_corpus)\n",
    "\n",
    "subsampled_indexed_corpus = tools.subsample_corpus(indexed_corpus, unigram_subsampling, t)\n",
    "\n",
    "sum([len(doc) for doc in subsampled_indexed_corpus])\n",
    "\n",
    "len(flattened_indexed_corpus)\n",
    "\n",
    "corpus_center_words, corpus_context_words = tools.get_corpus_centers_contexts(subsampled_indexed_corpus, window)\n",
    "\n",
    "unigram = tools.make_unigram_dict(corpus_center_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12784])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_center_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## w2v ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_prince = list(princeton_data.loc[0:princeton_data.shape[0],6])\n",
    "# vocabulary, tokenized_corpus, word2idx, idx2word = tools.tokenize_corpus(princeton_data[\"article\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(540690, 954250)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### word2vec ###\n",
    "\n",
    "w2v = Word2Vec(sample = t, sg=1, negative=1, size=30, window=2, min_count=1, max_vocab_size=None, seed=1, workers=1)\n",
    "\n",
    "w2v.build_vocab(tokenized_corpus)  # prepare the model vocabulary\n",
    "w2v.train(tokenized_corpus, total_examples=w2v.corpus_count, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"barbara\" in w2v.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('low', 0.6997503042221069),\n",
       " ('underprivileged', 0.6755244135856628),\n",
       " ('form', 0.6425948739051819),\n",
       " ('reality', 0.6398849487304688),\n",
       " ('being', 0.6124280691146851),\n",
       " ('annoyed', 0.5853123664855957),\n",
       " ('number', 0.5837895274162292),\n",
       " ('generation', 0.5814467668533325),\n",
       " ('altogether', 0.5752858519554138),\n",
       " ('first', 0.5721397399902344)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(\"income\") # assault, financial, mental, income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get corpus: 20 news groups\n",
    "\n",
    "# newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "# window = 5\n",
    "\n",
    "# vocabulary, tokenized_corpus, word2idx, idx2word = tools.tokenize_corpus(newsgroups_train.data)\n",
    "\n",
    "# indexed_corpus = tools.index_tokenized_corpus(tokenized_corpus, word2idx)\n",
    "\n",
    "# corpus_center_words, corpus_context_words = tools.get_corpus_centers_contexts(indexed_corpus, window)\n",
    "\n",
    "# unigram = tools.make_unigram_dict(corpus_center_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\"vocab_size\" : len(vocabulary), \"window\" : window, \n",
    "             \"batch_size\" : 200, \"optimizer\" : 80, \"learning_rate\" : 0.0005, \n",
    "             \"momentum\" : 0.99, \"num_epoch\" : 50, \"init_mult\" : 1, \n",
    "             \"variance\" : 0.995, \"start\" : True, \"nogpu\" : True, \n",
    "             \"embedding_dim\" : 300, \"freeze\" : False}\n",
    "\n",
    "args = SimpleNamespace(**args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss=50344.76953125\n",
      "Epoch 5, loss=26069.400390625\n",
      "Epoch 10, loss=16207.2314453125\n",
      "Epoch 15, loss=10887.6474609375\n",
      "Epoch 20, loss=7951.52392578125\n",
      "Epoch 25, loss=5915.08544921875\n",
      "Epoch 30, loss=4590.451171875\n",
      "Epoch 35, loss=3569.544677734375\n",
      "Epoch 40, loss=2941.241455078125\n",
      "Epoch 45, loss=2436.4189453125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = bsg.BSG(args.window, unigram, args.vocab_size, input_dim = 50, latent_dim = 30)\n",
    "optimizer = torch.optim.Adam(model.parameters(), args.learning_rate, betas=(args.momentum, 0.999))\n",
    "model = bsg.train(model, args, optimizer, corpus_center_words, corpus_context_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'lprofencoder'. \n"
     ]
    }
   ],
   "source": [
    "# %lprun -T lprofencoder -f bsg.BSG.encoder model.encoder(corpus_center_words[0:2], corpus_context_words[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder(corpus_center_words[0:2], corpus_context_words[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, \"bsg_prince_latent30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"bsg_prince_50e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx2word.get(corpus_center_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "types, counts = np.unique(corpus_center_words.tolist(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx2realidx = {idx: realidx for (idx , realidx) in enumerate(types)}\n",
    "test_realidx2idx = {realidx: idx for (idx , realidx) in enumerate(types)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_sim_matrix = cosine_similarity(model.type_means(torch.tensor(types)).detach().numpy(), \n",
    "                                   model.type_means(torch.tensor(types)).detach().numpy())\n",
    "def n_closest_words(word, cos_sim_matrix, n):\n",
    "    word_index = test_realidx2idx.get(word2idx.get(word))\n",
    "    print(\"word_index is \" + str(word_index))\n",
    "    close_words_indices = np.argsort(cos_sim_matrix[word_index])[-n:]\n",
    "    print(\"close_words_indices.shape\" + str(close_words_indices.shape))\n",
    "    return [idx2word.get(test_idx2realidx.get(j)) for j in close_words_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index is None\n",
      "close_words_indices.shape(1, 1282, 1282)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-ea7d75603aba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn_closest_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fraternity\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos_sim_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-69a623493dfe>\u001b[0m in \u001b[0;36mn_closest_words\u001b[0;34m(word, cos_sim_matrix, n)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mclose_words_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcos_sim_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"close_words_indices.shape\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose_words_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_idx2realidx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclose_words_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-69a623493dfe>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mclose_words_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcos_sim_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"close_words_indices.shape\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose_words_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_idx2realidx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclose_words_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "n_closest_words(\"fraternity\", cos_sim_matrix, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(list(test_idx2realidx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.type_means.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:math689env]",
   "language": "python",
   "name": "conda-env-math689env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
