{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kristynpantoja/bsg_pytorch/blob/master/BSGcollab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1chxO5BA_thF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# import gensim.downloader as api\n",
    "# from gensim.models import Word2Vec, FastText, KeyedVectors\n",
    "# from os.path import isfile\n",
    "\n",
    "import random\n",
    "random.seed(1234)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ILalDwY8IvlV",
    "outputId": "4c1dfc0c-b34f-437b-b5b2-f44b3bbd3b29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kristyn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/kristyn/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kgg_RNq-AGna"
   },
   "outputs": [],
   "source": [
    "# preprocessing stuff\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words): # preprocessing step\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "#     words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5Ze_s59Ckvr"
   },
   "outputs": [],
   "source": [
    "# Get corpus: 20 news groups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "tokenizer = RegexpTokenizer(u'(?ui)\\\\b[a-z]{3,}\\\\b')\n",
    "\n",
    "tokenized_corpus = [tokenizer.tokenize(document) for document in newsgroups_train.data]\n",
    "\n",
    "# normalize_corpus = [normalize(document) for document in tokenized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YeZLkgV-Bz2j"
   },
   "outputs": [],
   "source": [
    "flattened = [word for document in tokenized_corpus for word in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_pDDYfqC18q"
   },
   "outputs": [],
   "source": [
    "vocab = set(flattened)\n",
    "vocab = list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3ebaSFmC7rA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104685"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JzgbYAuOFavG"
   },
   "outputs": [],
   "source": [
    "# # get vocabulary: with some preprocessing\n",
    "# vectorizer = CountVectorizer(stop_words = 'english', \n",
    "#                              min_df=.01, max_df=0.9, \n",
    "#                              token_pattern = u'(?ui)\\\\b[a-z]{3,}\\\\b')\n",
    "\n",
    "# count_vecs = vectorizer.fit_transform(newsgroups_train.data)\n",
    "\n",
    "# # doc_term_matrix = count_vecs.toarray()\n",
    "# # doc_term_matrix.shape # number of documents, number of words (in vocab)\n",
    "\n",
    "# # note: vectorizer.get_feature_names() != vectorizer.vocabulary_\n",
    "\n",
    "# vocabulary = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "m86kafoIFxB1",
    "outputId": "159aad0c-7c05-43c0-b4aa-3448697475da"
   },
   "outputs": [],
   "source": [
    "# # vocabulary = []\n",
    "# # for sentence in tokenized_corpus:\n",
    "# #     for token in sentence:\n",
    "# #         if token not in vocabulary:\n",
    "# #             vocabulary.append(token)\n",
    "\n",
    "# word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "# idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "# vocabulary_size = len(vocabulary)\n",
    "\n",
    "\n",
    "# window_size = 2\n",
    "# idx_pairs = []\n",
    "# # for each sentence\n",
    "# for sentence in tokenized_corpus:\n",
    "#     indices = [word2idx[word] for word in sentence]\n",
    "#     # for each word, threated as center word\n",
    "#     for center_word_pos in range(len(indices)):\n",
    "#         # for each window position\n",
    "#         for w in range(-window_size, window_size + 1):\n",
    "#             context_word_pos = center_word_pos + w\n",
    "#             # make soure not jump out sentence\n",
    "#             if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "#                 continue\n",
    "#             context_word_idx = indices[context_word_pos]\n",
    "#             idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "# idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "v-cZqtyn9mj6",
    "outputId": "d94a233f-9dbd-428f-fd04-252c4a3126ca"
   },
   "outputs": [],
   "source": [
    "ragged_arrays = []\n",
    "# for each document\n",
    "for document in tokenized_corpus:\n",
    "    ragged_array = []\n",
    "    # for each word\n",
    "    for word in document:\n",
    "      ragged_arrays.append(word2idx[word])\n",
    "    ragged_arrays.append(ragged_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "y1KzDkliJDmg",
    "outputId": "fb72dd79-a0c8-48ff-f79a-6b3ba3792722"
   },
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_['lerxst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HnwGjRNfJFnn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "BSGcollab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
