{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kristynpantoja/bsg_pytorch/blob/master/BSGcollab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1chxO5BA_thF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# import gensim.downloader as api\n",
    "# from gensim.models import Word2Vec, FastText, KeyedVectors\n",
    "# from os.path import isfile\n",
    "\n",
    "import random\n",
    "random.seed(1234)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments \n",
    "\n",
    "\n",
    "\n",
    "window = 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ILalDwY8IvlV",
    "outputId": "4c1dfc0c-b34f-437b-b5b2-f44b3bbd3b29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kristyn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/kristyn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kgg_RNq-AGna"
   },
   "outputs": [],
   "source": [
    "# # preprocessing stuff\n",
    "\n",
    "# def remove_non_ascii(words):\n",
    "#     \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "#         new_words.append(new_word)\n",
    "#     return new_words\n",
    "\n",
    "# def to_lowercase(words):\n",
    "#     \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         new_word = word.lower()\n",
    "#         new_words.append(new_word)\n",
    "#     return new_words\n",
    "\n",
    "# def remove_punctuation(words):\n",
    "#     \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "#         if new_word != '':\n",
    "#             new_words.append(new_word)\n",
    "#     return new_words\n",
    "\n",
    "# def remove_stopwords(words):\n",
    "#     \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         if word not in stopwords.words('english'):\n",
    "#             new_words.append(word)\n",
    "#     return new_words\n",
    "\n",
    "# def stem_words(words):\n",
    "#     \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "#     stemmer = LancasterStemmer()\n",
    "#     stems = []\n",
    "#     for word in words:\n",
    "#         stem = stemmer.stem(word)\n",
    "#         stems.append(stem)\n",
    "#     return stems\n",
    "\n",
    "# def lemmatize_verbs(words):\n",
    "#     \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     lemmas = []\n",
    "#     for word in words:\n",
    "#         lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "#         lemmas.append(lemma)\n",
    "#     return lemmas\n",
    "\n",
    "# def normalize(words): # preprocessing step\n",
    "#     words = remove_non_ascii(words)\n",
    "#     words = to_lowercase(words)\n",
    "#     words = remove_punctuation(words)\n",
    "# #     words = remove_stopwords(words)\n",
    "#     return words\n",
    "\n",
    "# would be faster to preprocess before tokenizing, I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5Ze_s59Ckvr"
   },
   "outputs": [],
   "source": [
    "# Get corpus: 20 news groups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "# tokenize and preprocess\n",
    "\n",
    "tokenizer = RegexpTokenizer(u'(?ui)\\\\b[a-z]{3,}\\\\b')\n",
    "\n",
    "tokenized_corpus = [tokenizer.tokenize(document.lower()) for document in newsgroups_train.data]\n",
    "\n",
    "# normalize_corpus = [normalize(document) for document in tokenized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YeZLkgV-Bz2j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81072"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get vocabulary\n",
    "\n",
    "flattened = [word for document in tokenized_corpus for word in document]\n",
    "vocabulary = set(flattened)\n",
    "vocabulary = list(vocabulary)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JzgbYAuOFavG"
   },
   "outputs": [],
   "source": [
    "# # get vocabulary: with some preprocessing\n",
    "# vectorizer = CountVectorizer(stop_words = 'english', \n",
    "#                              min_df=.01, max_df=0.9, \n",
    "#                              token_pattern = u'(?ui)\\\\b[a-z]{3,}\\\\b')\n",
    "\n",
    "# count_vecs = vectorizer.fit_transform(newsgroups_train.data)\n",
    "\n",
    "# # doc_term_matrix = count_vecs.toarray()\n",
    "# # doc_term_matrix.shape # number of documents, number of words (in vocab)\n",
    "\n",
    "# # note: vectorizer.get_feature_names() != vectorizer.vocabulary_\n",
    "\n",
    "# vocabulary = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "m86kafoIFxB1",
    "outputId": "159aad0c-7c05-43c0-b4aa-3448697475da"
   },
   "outputs": [],
   "source": [
    "# # vocabulary = []\n",
    "# # for sentence in tokenized_corpus:\n",
    "# #     for token in sentence:\n",
    "# #         if token not in vocabulary:\n",
    "# #             vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx + 1 for (idx , w) in enumerate(vocabulary)}\n",
    "idx2word = {idx + 1: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "# idx_pairs = []\n",
    "# # for each sentence\n",
    "# for sentence in tokenized_corpus:\n",
    "#     indices = [word2idx[word] for word in sentence]\n",
    "#     # for each word, threated as center word\n",
    "#     for center_word_pos in range(len(indices)):\n",
    "#         # for each window position\n",
    "#         for w in range(-window_size, window_size + 1):\n",
    "#             context_word_pos = center_word_pos + w\n",
    "#             # make soure not jump out sentence\n",
    "#             if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "#                 continue\n",
    "#             context_word_idx = indices[context_word_pos]\n",
    "#             idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "# idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "v-cZqtyn9mj6",
    "outputId": "d94a233f-9dbd-428f-fd04-252c4a3126ca"
   },
   "outputs": [],
   "source": [
    "indexed_corpus = []\n",
    "# for each document\n",
    "for document in tokenized_corpus:\n",
    "    ragged_array = []\n",
    "    # for each word\n",
    "    for word in document:\n",
    "        ragged_array.append(word2idx[word])\n",
    "    indexed_corpus.append(ragged_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "y1KzDkliJDmg",
    "outputId": "fb72dd79-a0c8-48ff-f79a-6b3ba3792722"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'lerxst', 'wam', 'umd', 'edu', 'where', 'thing', 'subject', 'what', 'car', 'this', 'nntp', 'posting', 'host', 'wam', 'umd', 'edu', 'organization', 'university', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'anyone', 'out', 'there', 'could', 'enlighten', 'this', 'car', 'saw', 'the', 'other', 'day', 'was', 'door', 'sports', 'car', 'looked', 'from', 'the', 'late', 'early', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'the', 'body', 'this', 'all', 'know', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'where', 'this', 'car', 'made', 'history', 'whatever', 'info', 'you', 'have', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'brought', 'you', 'your', 'neighborhood', 'lerxst']\n",
      "97\n",
      "[78448, 76643, 8341, 61107, 11216, 44509, 6544, 59970, 71648, 41898, 46184, 52833, 33033, 70083, 8341, 61107, 11216, 67517, 32560, 52327, 35572, 28150, 35811, 54779, 55247, 55823, 53054, 45688, 14352, 72340, 46184, 41898, 52948, 13468, 22816, 10371, 54779, 13931, 79462, 41898, 20817, 78448, 13468, 14074, 10331, 54779, 24786, 10833, 13468, 12834, 26739, 66602, 48261, 30054, 13468, 6310, 13749, 54779, 26931, 78448, 13468, 35761, 13468, 42671, 46184, 64612, 56601, 55823, 25734, 46800, 7313, 46382, 80707, 1513, 18684, 62633, 44509, 46184, 41898, 45769, 80559, 79237, 45502, 19634, 45180, 46184, 48473, 26767, 41898, 39163, 68034, 65412, 70832, 19634, 40506, 58495, 76643]\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_corpus[0])\n",
    "print(len(tokenized_corpus[0]))\n",
    "print(indexed_corpus[0])\n",
    "print(len(indexed_corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HnwGjRNfJFnn"
   },
   "outputs": [],
   "source": [
    "padded_corpus = [np.pad(document, (window,window), 'constant', constant_values=(0, 0)) \n",
    "                 for document in indexed_corpus]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0, 78448, 76643,  8341, 61107, 11216, 44509,  6544,\n",
       "       59970, 71648, 41898, 46184, 52833, 33033, 70083,  8341, 61107,\n",
       "       11216, 67517, 32560, 52327, 35572, 28150, 35811, 54779, 55247,\n",
       "       55823, 53054, 45688, 14352, 72340, 46184, 41898, 52948, 13468,\n",
       "       22816, 10371, 54779, 13931, 79462, 41898, 20817, 78448, 13468,\n",
       "       14074, 10331, 54779, 24786, 10833, 13468, 12834, 26739, 66602,\n",
       "       48261, 30054, 13468,  6310, 13749, 54779, 26931, 78448, 13468,\n",
       "       35761, 13468, 42671, 46184, 64612, 56601, 55823, 25734, 46800,\n",
       "        7313, 46382, 80707,  1513, 18684, 62633, 44509, 46184, 41898,\n",
       "       45769, 80559, 79237, 45502, 19634, 45180, 46184, 48473, 26767,\n",
       "       41898, 39163, 68034, 65412, 70832, 19634, 40506, 58495, 76643,\n",
       "           0,     0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test\n",
    "\n",
    "test = padded_corpus[0]\n",
    "contexts = []\n",
    "for context in range(window, len(test) - window):\n",
    "    center_word = test[context]\n",
    "    context_words = np.concatenate((test[(context - window) : context], \n",
    "                                    test[context + 1 : context + window + 1]))\n",
    "    contexts.append((center_word, context_words))\n",
    "print(len(contexts))\n",
    "print(len(indexed_corpus[0]))\n",
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(78448, array([    0,     0, 76643,  8341])),\n",
       " (76643, array([    0, 78448,  8341, 61107])),\n",
       " (8341, array([78448, 76643, 61107, 11216])),\n",
       " (61107, array([76643,  8341, 11216, 44509])),\n",
       " (11216, array([ 8341, 61107, 44509,  6544])),\n",
       " (44509, array([61107, 11216,  6544, 59970])),\n",
       " (6544, array([11216, 44509, 59970, 71648])),\n",
       " (59970, array([44509,  6544, 71648, 41898])),\n",
       " (71648, array([ 6544, 59970, 41898, 46184])),\n",
       " (41898, array([59970, 71648, 46184, 52833])),\n",
       " (46184, array([71648, 41898, 52833, 33033])),\n",
       " (52833, array([41898, 46184, 33033, 70083])),\n",
       " (33033, array([46184, 52833, 70083,  8341])),\n",
       " (70083, array([52833, 33033,  8341, 61107])),\n",
       " (8341, array([33033, 70083, 61107, 11216])),\n",
       " (61107, array([70083,  8341, 11216, 67517])),\n",
       " (11216, array([ 8341, 61107, 67517, 32560])),\n",
       " (67517, array([61107, 11216, 32560, 52327])),\n",
       " (32560, array([11216, 67517, 52327, 35572])),\n",
       " (52327, array([67517, 32560, 35572, 28150])),\n",
       " (35572, array([32560, 52327, 28150, 35811])),\n",
       " (28150, array([52327, 35572, 35811, 54779])),\n",
       " (35811, array([35572, 28150, 54779, 55247])),\n",
       " (54779, array([28150, 35811, 55247, 55823])),\n",
       " (55247, array([35811, 54779, 55823, 53054])),\n",
       " (55823, array([54779, 55247, 53054, 45688])),\n",
       " (53054, array([55247, 55823, 45688, 14352])),\n",
       " (45688, array([55823, 53054, 14352, 72340])),\n",
       " (14352, array([53054, 45688, 72340, 46184])),\n",
       " (72340, array([45688, 14352, 46184, 41898])),\n",
       " (46184, array([14352, 72340, 41898, 52948])),\n",
       " (41898, array([72340, 46184, 52948, 13468])),\n",
       " (52948, array([46184, 41898, 13468, 22816])),\n",
       " (13468, array([41898, 52948, 22816, 10371])),\n",
       " (22816, array([52948, 13468, 10371, 54779])),\n",
       " (10371, array([13468, 22816, 54779, 13931])),\n",
       " (54779, array([22816, 10371, 13931, 79462])),\n",
       " (13931, array([10371, 54779, 79462, 41898])),\n",
       " (79462, array([54779, 13931, 41898, 20817])),\n",
       " (41898, array([13931, 79462, 20817, 78448])),\n",
       " (20817, array([79462, 41898, 78448, 13468])),\n",
       " (78448, array([41898, 20817, 13468, 14074])),\n",
       " (13468, array([20817, 78448, 14074, 10331])),\n",
       " (14074, array([78448, 13468, 10331, 54779])),\n",
       " (10331, array([13468, 14074, 54779, 24786])),\n",
       " (54779, array([14074, 10331, 24786, 10833])),\n",
       " (24786, array([10331, 54779, 10833, 13468])),\n",
       " (10833, array([54779, 24786, 13468, 12834])),\n",
       " (13468, array([24786, 10833, 12834, 26739])),\n",
       " (12834, array([10833, 13468, 26739, 66602])),\n",
       " (26739, array([13468, 12834, 66602, 48261])),\n",
       " (66602, array([12834, 26739, 48261, 30054])),\n",
       " (48261, array([26739, 66602, 30054, 13468])),\n",
       " (30054, array([66602, 48261, 13468,  6310])),\n",
       " (13468, array([48261, 30054,  6310, 13749])),\n",
       " (6310, array([30054, 13468, 13749, 54779])),\n",
       " (13749, array([13468,  6310, 54779, 26931])),\n",
       " (54779, array([ 6310, 13749, 26931, 78448])),\n",
       " (26931, array([13749, 54779, 78448, 13468])),\n",
       " (78448, array([54779, 26931, 13468, 35761])),\n",
       " (13468, array([26931, 78448, 35761, 13468])),\n",
       " (35761, array([78448, 13468, 13468, 42671])),\n",
       " (13468, array([13468, 35761, 42671, 46184])),\n",
       " (42671, array([35761, 13468, 46184, 64612])),\n",
       " (46184, array([13468, 42671, 64612, 56601])),\n",
       " (64612, array([42671, 46184, 56601, 55823])),\n",
       " (56601, array([46184, 64612, 55823, 25734])),\n",
       " (55823, array([64612, 56601, 25734, 46800])),\n",
       " (25734, array([56601, 55823, 46800,  7313])),\n",
       " (46800, array([55823, 25734,  7313, 46382])),\n",
       " (7313, array([25734, 46800, 46382, 80707])),\n",
       " (46382, array([46800,  7313, 80707,  1513])),\n",
       " (80707, array([ 7313, 46382,  1513, 18684])),\n",
       " (1513, array([46382, 80707, 18684, 62633])),\n",
       " (18684, array([80707,  1513, 62633, 44509])),\n",
       " (62633, array([ 1513, 18684, 44509, 46184])),\n",
       " (44509, array([18684, 62633, 46184, 41898])),\n",
       " (46184, array([62633, 44509, 41898, 45769])),\n",
       " (41898, array([44509, 46184, 45769, 80559])),\n",
       " (45769, array([46184, 41898, 80559, 79237])),\n",
       " (80559, array([41898, 45769, 79237, 45502])),\n",
       " (79237, array([45769, 80559, 45502, 19634])),\n",
       " (45502, array([80559, 79237, 19634, 45180])),\n",
       " (19634, array([79237, 45502, 45180, 46184])),\n",
       " (45180, array([45502, 19634, 46184, 48473])),\n",
       " (46184, array([19634, 45180, 48473, 26767])),\n",
       " (48473, array([45180, 46184, 26767, 41898])),\n",
       " (26767, array([46184, 48473, 41898, 39163])),\n",
       " (41898, array([48473, 26767, 39163, 68034])),\n",
       " (39163, array([26767, 41898, 68034, 65412])),\n",
       " (68034, array([41898, 39163, 65412, 70832])),\n",
       " (65412, array([39163, 68034, 70832, 19634])),\n",
       " (70832, array([68034, 65412, 19634, 40506])),\n",
       " (19634, array([65412, 70832, 40506, 58495])),\n",
       " (40506, array([70832, 19634, 58495, 76643])),\n",
       " (58495, array([19634, 40506, 76643,     0])),\n",
       " (76643, array([40506, 58495,     0,     0]))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = padded_corpus[0]\n",
    "contexts = []\n",
    "for context in range(window, len(test) - window):\n",
    "    center_word = test[context]\n",
    "    context_words = np.concatenate((test[(context - window) : context], \n",
    "                                    test[context + 1 : context + window + 1]))\n",
    "    contexts.append((center_word, context_words))\n",
    "print(len(contexts))\n",
    "print(len(indexed_corpus[0]))\n",
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_contexts(document, window):\n",
    "    contexts = []\n",
    "    for context in range(window, len(document) - window):\n",
    "        center_word = document[context]\n",
    "        context_words = np.concatenate((document[(context - window) : context], \n",
    "                                        document[(context + 1) : (context + window + 1)]))\n",
    "        contexts.append((center_word, context_words))\n",
    "    return contexts\n",
    "\n",
    "corpus_document_contexts = [document_contexts(document, window) for document in padded_corpus]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we just care about contexts right now, so flatten the list\n",
    "corpus_contexts = [pair for document_contexts in corpus_document_contexts for pair in document_contexts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "BSGcollab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
