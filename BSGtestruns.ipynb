{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kristynpantoja/bsg_pytorch/blob/master/BSGcollab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1chxO5BA_thF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# import gensim.downloader as api\n",
    "# from gensim.models import Word2Vec, FastText, KeyedVectors\n",
    "# from os.path import isfile\n",
    "\n",
    "import random\n",
    "random.seed(1234)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments \n",
    "\n",
    "\n",
    "\n",
    "window = 2\n",
    "# C = 2*window\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ILalDwY8IvlV",
    "outputId": "4c1dfc0c-b34f-437b-b5b2-f44b3bbd3b29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kristyn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/kristyn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kgg_RNq-AGna"
   },
   "outputs": [],
   "source": [
    "# # preprocessing stuff\n",
    "\n",
    "# def remove_non_ascii(words):\n",
    "#     \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "#         new_words.append(new_word)\n",
    "#     return new_words\n",
    "\n",
    "# def to_lowercase(words):\n",
    "#     \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         new_word = word.lower()\n",
    "#         new_words.append(new_word)\n",
    "#     return new_words\n",
    "\n",
    "# def remove_punctuation(words):\n",
    "#     \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "#         if new_word != '':\n",
    "#             new_words.append(new_word)\n",
    "#     return new_words\n",
    "\n",
    "# def remove_stopwords(words):\n",
    "#     \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         if word not in stopwords.words('english'):\n",
    "#             new_words.append(word)\n",
    "#     return new_words\n",
    "\n",
    "# def stem_words(words):\n",
    "#     \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "#     stemmer = LancasterStemmer()\n",
    "#     stems = []\n",
    "#     for word in words:\n",
    "#         stem = stemmer.stem(word)\n",
    "#         stems.append(stem)\n",
    "#     return stems\n",
    "\n",
    "# def lemmatize_verbs(words):\n",
    "#     \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     lemmas = []\n",
    "#     for word in words:\n",
    "#         lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "#         lemmas.append(lemma)\n",
    "#     return lemmas\n",
    "\n",
    "# def normalize(words): # preprocessing step\n",
    "#     words = remove_non_ascii(words)\n",
    "#     words = to_lowercase(words)\n",
    "#     words = remove_punctuation(words)\n",
    "# #     words = remove_stopwords(words)\n",
    "#     return words\n",
    "\n",
    "# would be faster to preprocess before tokenizing, I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5Ze_s59Ckvr"
   },
   "outputs": [],
   "source": [
    "# Get corpus: 20 news groups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "# tokenize and preprocess\n",
    "\n",
    "tokenizer = RegexpTokenizer(u'(?ui)\\\\b[a-z]{3,}\\\\b')\n",
    "\n",
    "tokenized_corpus = [tokenizer.tokenize(document.lower()) for document in newsgroups_train.data]\n",
    "\n",
    "# normalize_corpus = [normalize(document) for document in tokenized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YeZLkgV-Bz2j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81072"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get vocabulary\n",
    "\n",
    "flattened = [word for document in tokenized_corpus for word in document]\n",
    "vocabulary = set(flattened)\n",
    "vocabulary = list(vocabulary)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JzgbYAuOFavG"
   },
   "outputs": [],
   "source": [
    "# # get vocabulary: with some preprocessing\n",
    "# vectorizer = CountVectorizer(stop_words = 'english', \n",
    "#                              min_df=.01, max_df=0.9, \n",
    "#                              token_pattern = u'(?ui)\\\\b[a-z]{3,}\\\\b')\n",
    "\n",
    "# count_vecs = vectorizer.fit_transform(newsgroups_train.data)\n",
    "\n",
    "# # doc_term_matrix = count_vecs.toarray()\n",
    "# # doc_term_matrix.shape # number of documents, number of words (in vocab)\n",
    "\n",
    "# # note: vectorizer.get_feature_names() != vectorizer.vocabulary_\n",
    "\n",
    "# vocabulary = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "m86kafoIFxB1",
    "outputId": "159aad0c-7c05-43c0-b4aa-3448697475da"
   },
   "outputs": [],
   "source": [
    "# # vocabulary = []\n",
    "# # for sentence in tokenized_corpus:\n",
    "# #     for token in sentence:\n",
    "# #         if token not in vocabulary:\n",
    "# #             vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx + 1 for (idx , w) in enumerate(vocabulary)}\n",
    "idx2word = {idx + 1: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "# idx_pairs = []\n",
    "# # for each sentence\n",
    "# for sentence in tokenized_corpus:\n",
    "#     indices = [word2idx[word] for word in sentence]\n",
    "#     # for each word, threated as center word\n",
    "#     for center_word_pos in range(len(indices)):\n",
    "#         # for each window position\n",
    "#         for w in range(-window_size, window_size + 1):\n",
    "#             context_word_pos = center_word_pos + w\n",
    "#             # make soure not jump out sentence\n",
    "#             if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "#                 continue\n",
    "#             context_word_idx = indices[context_word_pos]\n",
    "#             idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "# idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "v-cZqtyn9mj6",
    "outputId": "d94a233f-9dbd-428f-fd04-252c4a3126ca"
   },
   "outputs": [],
   "source": [
    "indexed_corpus = []\n",
    "# for each document\n",
    "for document in tokenized_corpus:\n",
    "    ragged_array = []\n",
    "    # for each word\n",
    "    for word in document:\n",
    "        ragged_array.append(word2idx[word])\n",
    "    indexed_corpus.append(ragged_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "y1KzDkliJDmg",
    "outputId": "fb72dd79-a0c8-48ff-f79a-6b3ba3792722"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'lerxst', 'wam', 'umd', 'edu', 'where', 'thing', 'subject', 'what', 'car', 'this', 'nntp', 'posting', 'host', 'wam', 'umd', 'edu', 'organization', 'university', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'anyone', 'out', 'there', 'could', 'enlighten', 'this', 'car', 'saw', 'the', 'other', 'day', 'was', 'door', 'sports', 'car', 'looked', 'from', 'the', 'late', 'early', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'the', 'body', 'this', 'all', 'know', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'where', 'this', 'car', 'made', 'history', 'whatever', 'info', 'you', 'have', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'brought', 'you', 'your', 'neighborhood', 'lerxst']\n",
      "97\n",
      "[57285, 34984, 19554, 65487, 35763, 40534, 10469, 72279, 14752, 15136, 23427, 10421, 54449, 59986, 19554, 65487, 35763, 58082, 18949, 2950, 64667, 31071, 69639, 16134, 12941, 23446, 37382, 5343, 57955, 79346, 23427, 15136, 40648, 24013, 38054, 32959, 16134, 59843, 73803, 15136, 44393, 57285, 24013, 62400, 20111, 16134, 73963, 4852, 24013, 8504, 80835, 36405, 47888, 11804, 24013, 25310, 63794, 16134, 57439, 57285, 24013, 3132, 24013, 78148, 23427, 56325, 67273, 23446, 61016, 22516, 22468, 31402, 17238, 32349, 78938, 60954, 40534, 23427, 15136, 21052, 75748, 6595, 71511, 16688, 40059, 23427, 13520, 46765, 15136, 6757, 5046, 37414, 25046, 16688, 59908, 4551, 34984]\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_corpus[0])\n",
    "print(len(tokenized_corpus[0]))\n",
    "print(indexed_corpus[0])\n",
    "print(len(indexed_corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HnwGjRNfJFnn"
   },
   "outputs": [],
   "source": [
    "padded_corpus = [np.pad(document, (window,window), 'constant', constant_values=(0, 0)) \n",
    "                 for document in indexed_corpus]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(57285, array([    0,     0, 34984, 19554])),\n",
       " (34984, array([    0, 57285, 19554, 65487])),\n",
       " (19554, array([57285, 34984, 65487, 35763])),\n",
       " (65487, array([34984, 19554, 35763, 40534])),\n",
       " (35763, array([19554, 65487, 40534, 10469])),\n",
       " (40534, array([65487, 35763, 10469, 72279])),\n",
       " (10469, array([35763, 40534, 72279, 14752])),\n",
       " (72279, array([40534, 10469, 14752, 15136])),\n",
       " (14752, array([10469, 72279, 15136, 23427])),\n",
       " (15136, array([72279, 14752, 23427, 10421])),\n",
       " (23427, array([14752, 15136, 10421, 54449])),\n",
       " (10421, array([15136, 23427, 54449, 59986])),\n",
       " (54449, array([23427, 10421, 59986, 19554])),\n",
       " (59986, array([10421, 54449, 19554, 65487])),\n",
       " (19554, array([54449, 59986, 65487, 35763])),\n",
       " (65487, array([59986, 19554, 35763, 58082])),\n",
       " (35763, array([19554, 65487, 58082, 18949])),\n",
       " (58082, array([65487, 35763, 18949,  2950])),\n",
       " (18949, array([35763, 58082,  2950, 64667])),\n",
       " (2950, array([58082, 18949, 64667, 31071])),\n",
       " (64667, array([18949,  2950, 31071, 69639])),\n",
       " (31071, array([ 2950, 64667, 69639, 16134])),\n",
       " (69639, array([64667, 31071, 16134, 12941])),\n",
       " (16134, array([31071, 69639, 12941, 23446])),\n",
       " (12941, array([69639, 16134, 23446, 37382])),\n",
       " (23446, array([16134, 12941, 37382,  5343])),\n",
       " (37382, array([12941, 23446,  5343, 57955])),\n",
       " (5343, array([23446, 37382, 57955, 79346])),\n",
       " (57955, array([37382,  5343, 79346, 23427])),\n",
       " (79346, array([ 5343, 57955, 23427, 15136])),\n",
       " (23427, array([57955, 79346, 15136, 40648])),\n",
       " (15136, array([79346, 23427, 40648, 24013])),\n",
       " (40648, array([23427, 15136, 24013, 38054])),\n",
       " (24013, array([15136, 40648, 38054, 32959])),\n",
       " (38054, array([40648, 24013, 32959, 16134])),\n",
       " (32959, array([24013, 38054, 16134, 59843])),\n",
       " (16134, array([38054, 32959, 59843, 73803])),\n",
       " (59843, array([32959, 16134, 73803, 15136])),\n",
       " (73803, array([16134, 59843, 15136, 44393])),\n",
       " (15136, array([59843, 73803, 44393, 57285])),\n",
       " (44393, array([73803, 15136, 57285, 24013])),\n",
       " (57285, array([15136, 44393, 24013, 62400])),\n",
       " (24013, array([44393, 57285, 62400, 20111])),\n",
       " (62400, array([57285, 24013, 20111, 16134])),\n",
       " (20111, array([24013, 62400, 16134, 73963])),\n",
       " (16134, array([62400, 20111, 73963,  4852])),\n",
       " (73963, array([20111, 16134,  4852, 24013])),\n",
       " (4852, array([16134, 73963, 24013,  8504])),\n",
       " (24013, array([73963,  4852,  8504, 80835])),\n",
       " (8504, array([ 4852, 24013, 80835, 36405])),\n",
       " (80835, array([24013,  8504, 36405, 47888])),\n",
       " (36405, array([ 8504, 80835, 47888, 11804])),\n",
       " (47888, array([80835, 36405, 11804, 24013])),\n",
       " (11804, array([36405, 47888, 24013, 25310])),\n",
       " (24013, array([47888, 11804, 25310, 63794])),\n",
       " (25310, array([11804, 24013, 63794, 16134])),\n",
       " (63794, array([24013, 25310, 16134, 57439])),\n",
       " (16134, array([25310, 63794, 57439, 57285])),\n",
       " (57439, array([63794, 16134, 57285, 24013])),\n",
       " (57285, array([16134, 57439, 24013,  3132])),\n",
       " (24013, array([57439, 57285,  3132, 24013])),\n",
       " (3132, array([57285, 24013, 24013, 78148])),\n",
       " (24013, array([24013,  3132, 78148, 23427])),\n",
       " (78148, array([ 3132, 24013, 23427, 56325])),\n",
       " (23427, array([24013, 78148, 56325, 67273])),\n",
       " (56325, array([78148, 23427, 67273, 23446])),\n",
       " (67273, array([23427, 56325, 23446, 61016])),\n",
       " (23446, array([56325, 67273, 61016, 22516])),\n",
       " (61016, array([67273, 23446, 22516, 22468])),\n",
       " (22516, array([23446, 61016, 22468, 31402])),\n",
       " (22468, array([61016, 22516, 31402, 17238])),\n",
       " (31402, array([22516, 22468, 17238, 32349])),\n",
       " (17238, array([22468, 31402, 32349, 78938])),\n",
       " (32349, array([31402, 17238, 78938, 60954])),\n",
       " (78938, array([17238, 32349, 60954, 40534])),\n",
       " (60954, array([32349, 78938, 40534, 23427])),\n",
       " (40534, array([78938, 60954, 23427, 15136])),\n",
       " (23427, array([60954, 40534, 15136, 21052])),\n",
       " (15136, array([40534, 23427, 21052, 75748])),\n",
       " (21052, array([23427, 15136, 75748,  6595])),\n",
       " (75748, array([15136, 21052,  6595, 71511])),\n",
       " (6595, array([21052, 75748, 71511, 16688])),\n",
       " (71511, array([75748,  6595, 16688, 40059])),\n",
       " (16688, array([ 6595, 71511, 40059, 23427])),\n",
       " (40059, array([71511, 16688, 23427, 13520])),\n",
       " (23427, array([16688, 40059, 13520, 46765])),\n",
       " (13520, array([40059, 23427, 46765, 15136])),\n",
       " (46765, array([23427, 13520, 15136,  6757])),\n",
       " (15136, array([13520, 46765,  6757,  5046])),\n",
       " (6757, array([46765, 15136,  5046, 37414])),\n",
       " (5046, array([15136,  6757, 37414, 25046])),\n",
       " (37414, array([ 6757,  5046, 25046, 16688])),\n",
       " (25046, array([ 5046, 37414, 16688, 59908])),\n",
       " (16688, array([37414, 25046, 59908,  4551])),\n",
       " (59908, array([25046, 16688,  4551, 34984])),\n",
       " (4551, array([16688, 59908, 34984,     0])),\n",
       " (34984, array([59908,  4551,     0,     0]))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = padded_corpus[0]\n",
    "contexts = []\n",
    "for context in range(window, len(test) - window):\n",
    "    center_word = test[context]\n",
    "    context_words = np.concatenate((test[(context - window) : context], \n",
    "                                    test[context + 1 : context + window + 1]))\n",
    "    contexts.append((center_word, context_words))\n",
    "print(len(contexts))\n",
    "print(len(indexed_corpus[0]))\n",
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the contexts for each word by looking in each document\n",
    "\n",
    "def document_contexts(document, window):\n",
    "    contexts = []\n",
    "    for context in range(window, len(document) - window):\n",
    "        center_word = document[context]\n",
    "        context_words = np.concatenate((document[(context - window) : context], \n",
    "                                        document[(context + 1) : (context + window + 1)]))\n",
    "        contexts.append((center_word, context_words))\n",
    "    return contexts\n",
    "\n",
    "corpus_document_contexts = [document_contexts(document, window) for document in padded_corpus]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we just care about contexts right now, so flatten the list\n",
    "corpus_centers_contexts = [pair for document_contexts in corpus_document_contexts for pair in document_contexts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(57285, array([    0,     0, 34984, 19554])),\n",
       " (34984, array([    0, 57285, 19554, 65487])),\n",
       " (19554, array([57285, 34984, 65487, 35763]))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_centers_contexts[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_centers_contexts = list(map(list, zip(*corpus_centers_contexts)))\n",
    "\n",
    "corpus_center_words = torch.tensor(corpus_centers_contexts[0], dtype = torch.int64)\n",
    "corpus_context_words = torch.tensor(corpus_centers_contexts[1], dtype = torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([57285, 34984, 19554])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_center_words[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_center_words_for_negative_sampling = corpus_centers_contexts[0]\n",
    "corpus_centers, corpus_centers_counts = np.unique(corpus_center_words_for_negative_sampling, \n",
    "                                                  return_counts=True)\n",
    "unigram_dict = dict(zip(corpus_centers, corpus_centers_counts/len(corpus_center_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSG(nn.Module):\n",
    "    def __init__(self, unigram_dict, vocab_size, input_dim=50, hidden_dim=50, latent_dim=100, margin=1., model_name='BSG with the hinge loss'):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        :param vocab_size: the number of unique words\n",
    "        :param input_dim: the number of components in the encoder's word embeddings\n",
    "        :param hidden_dim: the number of components in the encoder's hidden layer\n",
    "        :param latent_dim: the number of components in the latent vector(also output word mu's)\n",
    "        :param margin: margin constant present in the hinge loss\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.margin = margin\n",
    "        \n",
    "        self.unigram_dist = torch.distributions.Categorical(torch.tensor(list(unigram_dict.values())))\n",
    "\n",
    "        # assign full parameters\n",
    "#         self.params_full = self.__build_model()\n",
    "\n",
    "        # extract only the actual parameter data-structures(tensors) as those will be optimized\n",
    "#         self.params = [param.value for param in self.params_full.values()]\n",
    "\n",
    "        # user accessible functions build ( e.g. training functions)\n",
    "#         self.__build_functions()\n",
    "\n",
    "        # encoder layers\n",
    "        self.encoder_embedding = nn.Embedding(self.vocab_size, self.input_dim, padding_idx = 0) # R\n",
    "        self.encoder_lin1 = nn.Linear(self.input_dim*2, self.hidden_dim) # M\n",
    "        self.encoder_mu = nn.Linear(self.hidden_dim, self.latent_dim) # U -> mu\n",
    "        self.encoder_logsigma = nn.Linear(self.hidden_dim, 1) # W -> log sigma\n",
    "        \n",
    "        # word embeddings' parameters for normal distributions of word types\n",
    "        self.type_means = nn.Embedding(self.vocab_size, self.latent_dim)\n",
    "        self.type_logvars = nn.Embedding(self.vocab_size, 1)\n",
    "        \n",
    "        \n",
    "    def encoder(self, centers_batch, contexts_batch):\n",
    "#             batch_size = centers_batch.shape[0]\n",
    "        sums = []\n",
    "        for center, context in zip(centers_batch, contexts_batch):\n",
    "            embed_center = self.encoder_embedding(center)\n",
    "            embed_context = self.encoder_embedding(context)\n",
    "            assert embed_context.shape[1] == self.input_dim, \"context embedding is not a 2d tensor\"\n",
    "            center_repeats = embed_center.repeat(2*window, 1)\n",
    "            concat = torch.cat((embed_context, center_repeats),1)\n",
    "            sum_relu_en1 = F.relu(self.encoder_lin1(concat)).sum(0) # a vector\n",
    "            sums.append(sum_relu_en1) # the vectors of sums\n",
    "        sums = torch.stack(sums)\n",
    "        mu = self.encoder_mu(sums)\n",
    "        logsigma = self.encoder_logsigma(sums)\n",
    "        return mu, logsigma\n",
    "    \n",
    "    def reparameterize(self, centers_batch, posterior_mean, posterior_logvar):\n",
    "        eps = Variable(centers_batch.data.new().resize_as_(posterior_mean.data).normal_())\n",
    "        z = posterior_mean + posterior_logvar.exp().sqrt() * eps\n",
    "        return z\n",
    "    \n",
    "    def KL(self, word_idx, post_mu, post_logsigma):\n",
    "        post_sigma = post_logsigma.exp()\n",
    "        type_mean = self.type_means(word_idx)\n",
    "        type_var = self.type_logvars(word_idx).exp().view([-1,post_logsigma.shape[1]])\n",
    "#         print(\"type_var\")\n",
    "#         print(type_var.shape)\n",
    "        var_division = post_sigma / type_var\n",
    "        diff = post_mu - type_mean\n",
    "        diff_term = (diff * diff).sum(1) / type_var ## added the .sum(1) from the original KL in our AVITM implementation of KL\n",
    "        logvar_division = type_var.log() - post_logsigma\n",
    "        # compute KL\n",
    "        KLD = 0.5 * ( (var_division + diff_term + logvar_division).sum(1) - self.latent_dim )\n",
    "        return KLD\n",
    "        \n",
    "    def forward(self, centers_batch, contexts_batch):\n",
    "#         print(\"contexts_batch.shape - should be batch size by context window size\")\n",
    "#         print(contexts_batch.shape)\n",
    "        mu, logsigma = model.encoder(centers_batch, contexts_batch)\n",
    "        # repeat mu, logsigma 2*window times, \n",
    "        #  one for each context word in an input\n",
    "        #  - do this for each input in the batch\n",
    "        mus = mu.repeat(1,window*2).view(-1,mu.shape[1]) \n",
    "        logsigmas = logsigma.repeat(1,window*2).view(-1,logsigma.shape[1])\n",
    "        # compute KLs\n",
    "        KL_contexts = self.KL(contexts_batch.view([-1,1]), mus, logsigmas)\n",
    "        negative_contexts_batch = self.unigram_dist.sample(contexts_batch.shape) + 1\n",
    "        KL_negative_contexts = self.KL(negative_contexts_batch.view([-1,1]), mus, logsigmas)\n",
    "        KL_center_word = self.KL(centers_batch, mu, logsigma)\n",
    "        \n",
    "        # compute hard margin of KLs of negative and positive context words\n",
    "        hard_margin_arg = KL_contexts - KL_negative_contexts + self.margin\n",
    "        loss = torch.max(hard_margin_arg, torch.zeros_like(hard_margin_arg)).sum() + KL_center_word.sum()\n",
    "        return loss / centers_batch.shape[0]\n",
    "    \n",
    "# b, window_size = pos_context_words.shape\n",
    "#         sigma_q = T.repeat(sigma_q, window_size, axis=0)\n",
    "#         mu_q = T.repeat(mu_q, window_size, axis=0)\n",
    "\n",
    "#         pos_c_resh = pos_context_words.reshape((-1, ))\n",
    "#         mu_p_pos, sigma_p_pos = self.__compute_prior_params(pos_c_resh)\n",
    "\n",
    "#         neg_c_resh = neg_context_words.reshape((-1, ))\n",
    "#         mu_p_neg, sigma_p_neg = self.__compute_prior_params(neg_c_resh)\n",
    "\n",
    "#         kl_pos = self.kl(mu_q, sigma_q, mu_p_pos, sigma_p_pos).reshape((b, -1))\n",
    "#         kl_neg = self.kl(mu_q, sigma_q, mu_p_neg, sigma_p_neg).reshape((b, -1))\n",
    "\n",
    "#         # hard margin\n",
    "#         return T.sum(T.maximum(0.0, self.margin - kl_neg + kl_pos) * mask, axis=1)\n",
    "    \n",
    "def train(model, args, optimizer, center_words, context_words):\n",
    "    '''\n",
    "    model - object of class BSG\n",
    "    args - dict of args\n",
    "    optimizer - nn.optim\n",
    "    centers_batch, contexts_batch\n",
    "    '''\n",
    "    for epoch in range(args.num_epoch):\n",
    "        all_indices = torch.randperm(context_words.size(0)).split(args.batch_size)\n",
    "        loss_epoch = 0.0\n",
    "        model.train()                   # switch to training mode\n",
    "        for batch_indices in all_indices:\n",
    "            if not args.nogpu: batch_indices = batch_indices.cuda()\n",
    "            context_words_input = Variable(context_words[batch_indices])\n",
    "            center_words_input = Variable(center_words[batch_indices])\n",
    "            loss = model(center_words_input, context_words_input)\n",
    "            # optimize\n",
    "            optimizer.zero_grad()       # clear previous gradients\n",
    "            loss.backward()             # backprop\n",
    "            optimizer.step()            # update parameters\n",
    "            # report\n",
    "            loss_epoch += loss.data[0]    # add loss to loss_epoch\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch {}, loss={}'.format(epoch, loss_epoch / len(all_indices)))\n",
    "#         model.losses.append(loss_epoch / len(all_indices))\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments for BSG model\n",
    "\n",
    "args_dict = {\"vocab_size\" : vocabulary_size, \"window\" : window, \n",
    "             \"batch_size\" : 200, \"optimizer\" : 80, \"learning_rate\" : 0.0005, \n",
    "             \"momentum\" : 0.99, \"num_epoch\" : 20, \"init_mult\" : 1, \n",
    "             \"variance\" : 0.995, \"start\" : True, \"nogpu\" : True, \n",
    "             \"embedding_dim\" : 300, \"freeze\" : False}\n",
    "\n",
    "args = SimpleNamespace(**args_dict)\n",
    "# args.num_input = doc_term_matrix_tensor.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BSG(unigram_dict, args.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, logsigma = model.encoder(corpus_center_words[0:2], corpus_context_words[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type_var\n",
      "torch.Size([8, 1])\n",
      "type_var\n",
      "torch.Size([8, 1])\n",
      "type_var\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "loss = model.forward(corpus_center_words[0:2], corpus_context_words[0:2]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4880.4453, grad_fn=<AddBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([78448, 76643])"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_center_words[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KLs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### repeat mus in the right order\n",
    "\n",
    "# mus = []\n",
    "# for row in mu:\n",
    "#     mus.append([row.repeat(window, 1)])\n",
    "# flattened = [word for document in mus for word in document]\n",
    "# torch.cat(flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_context_words[0:2].view([-1,1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     0, 76643,  8341,     0, 78448,  8341, 61107])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus_context_words[0:2].reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1328,  0.3626,  0.2231, -0.4561,  0.2015, -0.5421, -0.2960,  0.0136,\n",
       "          0.7432, -0.7658,  0.3957, -0.5349, -0.5035,  0.8872, -0.5797, -1.1881,\n",
       "          0.6368,  0.2594, -0.0732,  0.2054, -0.2497, -0.6358,  0.5502,  0.7387,\n",
       "          0.4454, -0.1407,  0.3270, -0.2328,  0.2671, -0.8467,  0.1583,  0.3733,\n",
       "          0.1205,  0.1361,  1.4395, -0.5683, -0.2840,  0.5197,  0.0914,  0.1981,\n",
       "          0.3870,  0.3187,  0.9277,  0.5754, -1.0530,  0.7603,  0.5672, -0.0465,\n",
       "          0.3120,  0.9033, -0.7251, -0.6650,  0.6739, -0.3370,  0.2263, -0.7380,\n",
       "         -0.5226,  0.2347,  0.2227,  0.2495, -0.3583,  0.1735,  0.6435,  0.8880,\n",
       "          0.1533, -0.2974, -0.6060,  0.6317,  0.3030, -0.1686,  0.0699,  1.2605,\n",
       "          0.4658,  0.7306, -0.1133,  0.5964, -0.7103,  0.2760,  0.2682,  0.4633,\n",
       "          0.6884, -0.1388,  0.8652, -0.6496, -0.4174, -0.0057,  0.3381, -0.0209,\n",
       "          0.6112,  0.5303,  1.3132, -0.4711,  0.1899, -0.5188, -0.7462, -1.6332,\n",
       "          0.1174,  0.1171,  0.0244, -1.0617],\n",
       "        [-0.2531,  0.1268, -0.8817,  0.0081, -0.0978, -0.5887, -0.3868,  0.5440,\n",
       "          0.3901,  0.4583, -0.6494,  0.9868,  1.9254,  0.5384, -0.3961,  0.1151,\n",
       "          1.4960, -0.7420,  0.8954, -0.4631, -1.4226, -1.2924,  0.6434,  1.9977,\n",
       "          0.7075,  1.6488,  0.5206, -0.1638, -0.1935, -0.0101, -0.0958,  0.3243,\n",
       "         -1.5562,  0.3891,  1.3261,  0.6362, -1.1861,  0.1591,  2.1919,  0.2574,\n",
       "          0.3116, -1.3195,  1.0111,  0.9251, -0.9357, -0.1138, -0.4483, -1.4078,\n",
       "          0.7055,  1.5067, -0.7060, -0.6469, -0.4500,  0.2224,  0.0491, -1.3370,\n",
       "         -0.5280, -0.1823, -1.0919,  0.5864, -0.7907, -1.3954,  0.0837, -1.5275,\n",
       "         -0.3566,  0.7349, -1.1680,  1.4366,  1.8719, -0.2708,  0.2722,  0.1338,\n",
       "          0.1279, -0.1885,  0.0821,  0.2917, -1.1117,  0.4906, -0.3542,  1.0290,\n",
       "          0.7155,  0.6591, -0.1743, -0.0099, -0.4158, -1.4385, -0.6939,  0.3654,\n",
       "          0.0597,  1.8207,  0.7687,  0.1065, -0.3376, -0.6472, -0.3812,  0.1891,\n",
       "         -0.2541, -0.2174,  0.9455, -0.6504]], grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1328,  0.3626,  0.2231, -0.4561,  0.2015, -0.5421, -0.2960,  0.0136,\n",
       "          0.7432, -0.7658,  0.3957, -0.5349, -0.5035,  0.8872, -0.5797, -1.1881,\n",
       "          0.6368,  0.2594, -0.0732,  0.2054, -0.2497, -0.6358,  0.5502,  0.7387,\n",
       "          0.4454, -0.1407,  0.3270, -0.2328,  0.2671, -0.8467,  0.1583,  0.3733,\n",
       "          0.1205,  0.1361,  1.4395, -0.5683, -0.2840,  0.5197,  0.0914,  0.1981,\n",
       "          0.3870,  0.3187,  0.9277,  0.5754, -1.0530,  0.7603,  0.5672, -0.0465,\n",
       "          0.3120,  0.9033, -0.7251, -0.6650,  0.6739, -0.3370,  0.2263, -0.7380,\n",
       "         -0.5226,  0.2347,  0.2227,  0.2495, -0.3583,  0.1735,  0.6435,  0.8880,\n",
       "          0.1533, -0.2974, -0.6060,  0.6317,  0.3030, -0.1686,  0.0699,  1.2605,\n",
       "          0.4658,  0.7306, -0.1133,  0.5964, -0.7103,  0.2760,  0.2682,  0.4633,\n",
       "          0.6884, -0.1388,  0.8652, -0.6496, -0.4174, -0.0057,  0.3381, -0.0209,\n",
       "          0.6112,  0.5303,  1.3132, -0.4711,  0.1899, -0.5188, -0.7462, -1.6332,\n",
       "          0.1174,  0.1171,  0.0244, -1.0617],\n",
       "        [-0.1328,  0.3626,  0.2231, -0.4561,  0.2015, -0.5421, -0.2960,  0.0136,\n",
       "          0.7432, -0.7658,  0.3957, -0.5349, -0.5035,  0.8872, -0.5797, -1.1881,\n",
       "          0.6368,  0.2594, -0.0732,  0.2054, -0.2497, -0.6358,  0.5502,  0.7387,\n",
       "          0.4454, -0.1407,  0.3270, -0.2328,  0.2671, -0.8467,  0.1583,  0.3733,\n",
       "          0.1205,  0.1361,  1.4395, -0.5683, -0.2840,  0.5197,  0.0914,  0.1981,\n",
       "          0.3870,  0.3187,  0.9277,  0.5754, -1.0530,  0.7603,  0.5672, -0.0465,\n",
       "          0.3120,  0.9033, -0.7251, -0.6650,  0.6739, -0.3370,  0.2263, -0.7380,\n",
       "         -0.5226,  0.2347,  0.2227,  0.2495, -0.3583,  0.1735,  0.6435,  0.8880,\n",
       "          0.1533, -0.2974, -0.6060,  0.6317,  0.3030, -0.1686,  0.0699,  1.2605,\n",
       "          0.4658,  0.7306, -0.1133,  0.5964, -0.7103,  0.2760,  0.2682,  0.4633,\n",
       "          0.6884, -0.1388,  0.8652, -0.6496, -0.4174, -0.0057,  0.3381, -0.0209,\n",
       "          0.6112,  0.5303,  1.3132, -0.4711,  0.1899, -0.5188, -0.7462, -1.6332,\n",
       "          0.1174,  0.1171,  0.0244, -1.0617],\n",
       "        [-0.1328,  0.3626,  0.2231, -0.4561,  0.2015, -0.5421, -0.2960,  0.0136,\n",
       "          0.7432, -0.7658,  0.3957, -0.5349, -0.5035,  0.8872, -0.5797, -1.1881,\n",
       "          0.6368,  0.2594, -0.0732,  0.2054, -0.2497, -0.6358,  0.5502,  0.7387,\n",
       "          0.4454, -0.1407,  0.3270, -0.2328,  0.2671, -0.8467,  0.1583,  0.3733,\n",
       "          0.1205,  0.1361,  1.4395, -0.5683, -0.2840,  0.5197,  0.0914,  0.1981,\n",
       "          0.3870,  0.3187,  0.9277,  0.5754, -1.0530,  0.7603,  0.5672, -0.0465,\n",
       "          0.3120,  0.9033, -0.7251, -0.6650,  0.6739, -0.3370,  0.2263, -0.7380,\n",
       "         -0.5226,  0.2347,  0.2227,  0.2495, -0.3583,  0.1735,  0.6435,  0.8880,\n",
       "          0.1533, -0.2974, -0.6060,  0.6317,  0.3030, -0.1686,  0.0699,  1.2605,\n",
       "          0.4658,  0.7306, -0.1133,  0.5964, -0.7103,  0.2760,  0.2682,  0.4633,\n",
       "          0.6884, -0.1388,  0.8652, -0.6496, -0.4174, -0.0057,  0.3381, -0.0209,\n",
       "          0.6112,  0.5303,  1.3132, -0.4711,  0.1899, -0.5188, -0.7462, -1.6332,\n",
       "          0.1174,  0.1171,  0.0244, -1.0617],\n",
       "        [-0.1328,  0.3626,  0.2231, -0.4561,  0.2015, -0.5421, -0.2960,  0.0136,\n",
       "          0.7432, -0.7658,  0.3957, -0.5349, -0.5035,  0.8872, -0.5797, -1.1881,\n",
       "          0.6368,  0.2594, -0.0732,  0.2054, -0.2497, -0.6358,  0.5502,  0.7387,\n",
       "          0.4454, -0.1407,  0.3270, -0.2328,  0.2671, -0.8467,  0.1583,  0.3733,\n",
       "          0.1205,  0.1361,  1.4395, -0.5683, -0.2840,  0.5197,  0.0914,  0.1981,\n",
       "          0.3870,  0.3187,  0.9277,  0.5754, -1.0530,  0.7603,  0.5672, -0.0465,\n",
       "          0.3120,  0.9033, -0.7251, -0.6650,  0.6739, -0.3370,  0.2263, -0.7380,\n",
       "         -0.5226,  0.2347,  0.2227,  0.2495, -0.3583,  0.1735,  0.6435,  0.8880,\n",
       "          0.1533, -0.2974, -0.6060,  0.6317,  0.3030, -0.1686,  0.0699,  1.2605,\n",
       "          0.4658,  0.7306, -0.1133,  0.5964, -0.7103,  0.2760,  0.2682,  0.4633,\n",
       "          0.6884, -0.1388,  0.8652, -0.6496, -0.4174, -0.0057,  0.3381, -0.0209,\n",
       "          0.6112,  0.5303,  1.3132, -0.4711,  0.1899, -0.5188, -0.7462, -1.6332,\n",
       "          0.1174,  0.1171,  0.0244, -1.0617],\n",
       "        [-0.2531,  0.1268, -0.8817,  0.0081, -0.0978, -0.5887, -0.3868,  0.5440,\n",
       "          0.3901,  0.4583, -0.6494,  0.9868,  1.9254,  0.5384, -0.3961,  0.1151,\n",
       "          1.4960, -0.7420,  0.8954, -0.4631, -1.4226, -1.2924,  0.6434,  1.9977,\n",
       "          0.7075,  1.6488,  0.5206, -0.1638, -0.1935, -0.0101, -0.0958,  0.3243,\n",
       "         -1.5562,  0.3891,  1.3261,  0.6362, -1.1861,  0.1591,  2.1919,  0.2574,\n",
       "          0.3116, -1.3195,  1.0111,  0.9251, -0.9357, -0.1138, -0.4483, -1.4078,\n",
       "          0.7055,  1.5067, -0.7060, -0.6469, -0.4500,  0.2224,  0.0491, -1.3370,\n",
       "         -0.5280, -0.1823, -1.0919,  0.5864, -0.7907, -1.3954,  0.0837, -1.5275,\n",
       "         -0.3566,  0.7349, -1.1680,  1.4366,  1.8719, -0.2708,  0.2722,  0.1338,\n",
       "          0.1279, -0.1885,  0.0821,  0.2917, -1.1117,  0.4906, -0.3542,  1.0290,\n",
       "          0.7155,  0.6591, -0.1743, -0.0099, -0.4158, -1.4385, -0.6939,  0.3654,\n",
       "          0.0597,  1.8207,  0.7687,  0.1065, -0.3376, -0.6472, -0.3812,  0.1891,\n",
       "         -0.2541, -0.2174,  0.9455, -0.6504],\n",
       "        [-0.2531,  0.1268, -0.8817,  0.0081, -0.0978, -0.5887, -0.3868,  0.5440,\n",
       "          0.3901,  0.4583, -0.6494,  0.9868,  1.9254,  0.5384, -0.3961,  0.1151,\n",
       "          1.4960, -0.7420,  0.8954, -0.4631, -1.4226, -1.2924,  0.6434,  1.9977,\n",
       "          0.7075,  1.6488,  0.5206, -0.1638, -0.1935, -0.0101, -0.0958,  0.3243,\n",
       "         -1.5562,  0.3891,  1.3261,  0.6362, -1.1861,  0.1591,  2.1919,  0.2574,\n",
       "          0.3116, -1.3195,  1.0111,  0.9251, -0.9357, -0.1138, -0.4483, -1.4078,\n",
       "          0.7055,  1.5067, -0.7060, -0.6469, -0.4500,  0.2224,  0.0491, -1.3370,\n",
       "         -0.5280, -0.1823, -1.0919,  0.5864, -0.7907, -1.3954,  0.0837, -1.5275,\n",
       "         -0.3566,  0.7349, -1.1680,  1.4366,  1.8719, -0.2708,  0.2722,  0.1338,\n",
       "          0.1279, -0.1885,  0.0821,  0.2917, -1.1117,  0.4906, -0.3542,  1.0290,\n",
       "          0.7155,  0.6591, -0.1743, -0.0099, -0.4158, -1.4385, -0.6939,  0.3654,\n",
       "          0.0597,  1.8207,  0.7687,  0.1065, -0.3376, -0.6472, -0.3812,  0.1891,\n",
       "         -0.2541, -0.2174,  0.9455, -0.6504],\n",
       "        [-0.2531,  0.1268, -0.8817,  0.0081, -0.0978, -0.5887, -0.3868,  0.5440,\n",
       "          0.3901,  0.4583, -0.6494,  0.9868,  1.9254,  0.5384, -0.3961,  0.1151,\n",
       "          1.4960, -0.7420,  0.8954, -0.4631, -1.4226, -1.2924,  0.6434,  1.9977,\n",
       "          0.7075,  1.6488,  0.5206, -0.1638, -0.1935, -0.0101, -0.0958,  0.3243,\n",
       "         -1.5562,  0.3891,  1.3261,  0.6362, -1.1861,  0.1591,  2.1919,  0.2574,\n",
       "          0.3116, -1.3195,  1.0111,  0.9251, -0.9357, -0.1138, -0.4483, -1.4078,\n",
       "          0.7055,  1.5067, -0.7060, -0.6469, -0.4500,  0.2224,  0.0491, -1.3370,\n",
       "         -0.5280, -0.1823, -1.0919,  0.5864, -0.7907, -1.3954,  0.0837, -1.5275,\n",
       "         -0.3566,  0.7349, -1.1680,  1.4366,  1.8719, -0.2708,  0.2722,  0.1338,\n",
       "          0.1279, -0.1885,  0.0821,  0.2917, -1.1117,  0.4906, -0.3542,  1.0290,\n",
       "          0.7155,  0.6591, -0.1743, -0.0099, -0.4158, -1.4385, -0.6939,  0.3654,\n",
       "          0.0597,  1.8207,  0.7687,  0.1065, -0.3376, -0.6472, -0.3812,  0.1891,\n",
       "         -0.2541, -0.2174,  0.9455, -0.6504],\n",
       "        [-0.2531,  0.1268, -0.8817,  0.0081, -0.0978, -0.5887, -0.3868,  0.5440,\n",
       "          0.3901,  0.4583, -0.6494,  0.9868,  1.9254,  0.5384, -0.3961,  0.1151,\n",
       "          1.4960, -0.7420,  0.8954, -0.4631, -1.4226, -1.2924,  0.6434,  1.9977,\n",
       "          0.7075,  1.6488,  0.5206, -0.1638, -0.1935, -0.0101, -0.0958,  0.3243,\n",
       "         -1.5562,  0.3891,  1.3261,  0.6362, -1.1861,  0.1591,  2.1919,  0.2574,\n",
       "          0.3116, -1.3195,  1.0111,  0.9251, -0.9357, -0.1138, -0.4483, -1.4078,\n",
       "          0.7055,  1.5067, -0.7060, -0.6469, -0.4500,  0.2224,  0.0491, -1.3370,\n",
       "         -0.5280, -0.1823, -1.0919,  0.5864, -0.7907, -1.3954,  0.0837, -1.5275,\n",
       "         -0.3566,  0.7349, -1.1680,  1.4366,  1.8719, -0.2708,  0.2722,  0.1338,\n",
       "          0.1279, -0.1885,  0.0821,  0.2917, -1.1117,  0.4906, -0.3542,  1.0290,\n",
       "          0.7155,  0.6591, -0.1743, -0.0099, -0.4158, -1.4385, -0.6939,  0.3654,\n",
       "          0.0597,  1.8207,  0.7687,  0.1065, -0.3376, -0.6472, -0.3812,  0.1891,\n",
       "         -0.2541, -0.2174,  0.9455, -0.6504]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu.repeat(1,window*2).view(-1,mu.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), args.learning_rate, betas=(args.momentum, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/math689env/lib/python3.7/site-packages/ipykernel/__main__.py:132: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss=234733.203125\n",
      "Epoch 5, loss=172145.453125\n",
      "Epoch 10, loss=151382.671875\n",
      "Epoch 15, loss=138616.09375\n"
     ]
    }
   ],
   "source": [
    "model = train(model, args, optimizer, corpus_center_words[0:1000], corpus_context_words[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "BSGcollab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
