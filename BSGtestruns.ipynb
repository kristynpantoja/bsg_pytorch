{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kristynpantoja/bsg_pytorch/blob/master/BSGcollab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1chxO5BA_thF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# import gensim.downloader as api\n",
    "# from gensim.models import Word2Vec, FastText, KeyedVectors\n",
    "# from os.path import isfile\n",
    "\n",
    "import random\n",
    "random.seed(1234)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments \n",
    "\n",
    "\n",
    "\n",
    "window = 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ILalDwY8IvlV",
    "outputId": "4c1dfc0c-b34f-437b-b5b2-f44b3bbd3b29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kristyn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/kristyn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kgg_RNq-AGna"
   },
   "outputs": [],
   "source": [
    "# # preprocessing stuff\n",
    "\n",
    "# def remove_non_ascii(words):\n",
    "#     \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "#         new_words.append(new_word)\n",
    "#     return new_words\n",
    "\n",
    "# def to_lowercase(words):\n",
    "#     \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         new_word = word.lower()\n",
    "#         new_words.append(new_word)\n",
    "#     return new_words\n",
    "\n",
    "# def remove_punctuation(words):\n",
    "#     \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "#         if new_word != '':\n",
    "#             new_words.append(new_word)\n",
    "#     return new_words\n",
    "\n",
    "# def remove_stopwords(words):\n",
    "#     \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         if word not in stopwords.words('english'):\n",
    "#             new_words.append(word)\n",
    "#     return new_words\n",
    "\n",
    "# def stem_words(words):\n",
    "#     \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "#     stemmer = LancasterStemmer()\n",
    "#     stems = []\n",
    "#     for word in words:\n",
    "#         stem = stemmer.stem(word)\n",
    "#         stems.append(stem)\n",
    "#     return stems\n",
    "\n",
    "# def lemmatize_verbs(words):\n",
    "#     \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     lemmas = []\n",
    "#     for word in words:\n",
    "#         lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "#         lemmas.append(lemma)\n",
    "#     return lemmas\n",
    "\n",
    "# def normalize(words): # preprocessing step\n",
    "#     words = remove_non_ascii(words)\n",
    "#     words = to_lowercase(words)\n",
    "#     words = remove_punctuation(words)\n",
    "# #     words = remove_stopwords(words)\n",
    "#     return words\n",
    "\n",
    "# would be faster to preprocess before tokenizing, I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5Ze_s59Ckvr"
   },
   "outputs": [],
   "source": [
    "# Get corpus: 20 news groups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "# tokenize and preprocess\n",
    "\n",
    "tokenizer = RegexpTokenizer(u'(?ui)\\\\b[a-z]{3,}\\\\b')\n",
    "\n",
    "tokenized_corpus = [tokenizer.tokenize(document.lower()) for document in newsgroups_train.data]\n",
    "\n",
    "# normalize_corpus = [normalize(document) for document in tokenized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YeZLkgV-Bz2j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81072"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get vocabulary\n",
    "\n",
    "flattened = [word for document in tokenized_corpus for word in document]\n",
    "vocabulary = set(flattened)\n",
    "vocabulary = list(vocabulary)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JzgbYAuOFavG"
   },
   "outputs": [],
   "source": [
    "# # get vocabulary: with some preprocessing\n",
    "# vectorizer = CountVectorizer(stop_words = 'english', \n",
    "#                              min_df=.01, max_df=0.9, \n",
    "#                              token_pattern = u'(?ui)\\\\b[a-z]{3,}\\\\b')\n",
    "\n",
    "# count_vecs = vectorizer.fit_transform(newsgroups_train.data)\n",
    "\n",
    "# # doc_term_matrix = count_vecs.toarray()\n",
    "# # doc_term_matrix.shape # number of documents, number of words (in vocab)\n",
    "\n",
    "# # note: vectorizer.get_feature_names() != vectorizer.vocabulary_\n",
    "\n",
    "# vocabulary = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "m86kafoIFxB1",
    "outputId": "159aad0c-7c05-43c0-b4aa-3448697475da"
   },
   "outputs": [],
   "source": [
    "# # vocabulary = []\n",
    "# # for sentence in tokenized_corpus:\n",
    "# #     for token in sentence:\n",
    "# #         if token not in vocabulary:\n",
    "# #             vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx + 1 for (idx , w) in enumerate(vocabulary)}\n",
    "idx2word = {idx + 1: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "# idx_pairs = []\n",
    "# # for each sentence\n",
    "# for sentence in tokenized_corpus:\n",
    "#     indices = [word2idx[word] for word in sentence]\n",
    "#     # for each word, threated as center word\n",
    "#     for center_word_pos in range(len(indices)):\n",
    "#         # for each window position\n",
    "#         for w in range(-window_size, window_size + 1):\n",
    "#             context_word_pos = center_word_pos + w\n",
    "#             # make soure not jump out sentence\n",
    "#             if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "#                 continue\n",
    "#             context_word_idx = indices[context_word_pos]\n",
    "#             idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "# idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "v-cZqtyn9mj6",
    "outputId": "d94a233f-9dbd-428f-fd04-252c4a3126ca"
   },
   "outputs": [],
   "source": [
    "indexed_corpus = []\n",
    "# for each document\n",
    "for document in tokenized_corpus:\n",
    "    ragged_array = []\n",
    "    # for each word\n",
    "    for word in document:\n",
    "        ragged_array.append(word2idx[word])\n",
    "    indexed_corpus.append(ragged_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "y1KzDkliJDmg",
    "outputId": "fb72dd79-a0c8-48ff-f79a-6b3ba3792722"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'lerxst', 'wam', 'umd', 'edu', 'where', 'thing', 'subject', 'what', 'car', 'this', 'nntp', 'posting', 'host', 'wam', 'umd', 'edu', 'organization', 'university', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'anyone', 'out', 'there', 'could', 'enlighten', 'this', 'car', 'saw', 'the', 'other', 'day', 'was', 'door', 'sports', 'car', 'looked', 'from', 'the', 'late', 'early', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'the', 'body', 'this', 'all', 'know', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'where', 'this', 'car', 'made', 'history', 'whatever', 'info', 'you', 'have', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'brought', 'you', 'your', 'neighborhood', 'lerxst']\n",
      "97\n",
      "[78448, 76643, 8341, 61107, 11216, 44509, 6544, 59970, 71648, 41898, 46184, 52833, 33033, 70083, 8341, 61107, 11216, 67517, 32560, 52327, 35572, 28150, 35811, 54779, 55247, 55823, 53054, 45688, 14352, 72340, 46184, 41898, 52948, 13468, 22816, 10371, 54779, 13931, 79462, 41898, 20817, 78448, 13468, 14074, 10331, 54779, 24786, 10833, 13468, 12834, 26739, 66602, 48261, 30054, 13468, 6310, 13749, 54779, 26931, 78448, 13468, 35761, 13468, 42671, 46184, 64612, 56601, 55823, 25734, 46800, 7313, 46382, 80707, 1513, 18684, 62633, 44509, 46184, 41898, 45769, 80559, 79237, 45502, 19634, 45180, 46184, 48473, 26767, 41898, 39163, 68034, 65412, 70832, 19634, 40506, 58495, 76643]\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_corpus[0])\n",
    "print(len(tokenized_corpus[0]))\n",
    "print(indexed_corpus[0])\n",
    "print(len(indexed_corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HnwGjRNfJFnn"
   },
   "outputs": [],
   "source": [
    "padded_corpus = [np.pad(document, (window,window), 'constant', constant_values=(0, 0)) \n",
    "                 for document in indexed_corpus]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0, 78448, 76643,  8341, 61107, 11216, 44509,  6544,\n",
       "       59970, 71648, 41898, 46184, 52833, 33033, 70083,  8341, 61107,\n",
       "       11216, 67517, 32560, 52327, 35572, 28150, 35811, 54779, 55247,\n",
       "       55823, 53054, 45688, 14352, 72340, 46184, 41898, 52948, 13468,\n",
       "       22816, 10371, 54779, 13931, 79462, 41898, 20817, 78448, 13468,\n",
       "       14074, 10331, 54779, 24786, 10833, 13468, 12834, 26739, 66602,\n",
       "       48261, 30054, 13468,  6310, 13749, 54779, 26931, 78448, 13468,\n",
       "       35761, 13468, 42671, 46184, 64612, 56601, 55823, 25734, 46800,\n",
       "        7313, 46382, 80707,  1513, 18684, 62633, 44509, 46184, 41898,\n",
       "       45769, 80559, 79237, 45502, 19634, 45180, 46184, 48473, 26767,\n",
       "       41898, 39163, 68034, 65412, 70832, 19634, 40506, 58495, 76643,\n",
       "           0,     0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test\n",
    "\n",
    "test = padded_corpus[0]\n",
    "contexts = []\n",
    "for context in range(window, len(test) - window):\n",
    "    center_word = test[context]\n",
    "    context_words = np.concatenate((test[(context - window) : context], \n",
    "                                    test[context + 1 : context + window + 1]))\n",
    "    contexts.append((center_word, context_words))\n",
    "print(len(contexts))\n",
    "print(len(indexed_corpus[0]))\n",
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(78448, array([    0,     0, 76643,  8341])),\n",
       " (76643, array([    0, 78448,  8341, 61107])),\n",
       " (8341, array([78448, 76643, 61107, 11216])),\n",
       " (61107, array([76643,  8341, 11216, 44509])),\n",
       " (11216, array([ 8341, 61107, 44509,  6544])),\n",
       " (44509, array([61107, 11216,  6544, 59970])),\n",
       " (6544, array([11216, 44509, 59970, 71648])),\n",
       " (59970, array([44509,  6544, 71648, 41898])),\n",
       " (71648, array([ 6544, 59970, 41898, 46184])),\n",
       " (41898, array([59970, 71648, 46184, 52833])),\n",
       " (46184, array([71648, 41898, 52833, 33033])),\n",
       " (52833, array([41898, 46184, 33033, 70083])),\n",
       " (33033, array([46184, 52833, 70083,  8341])),\n",
       " (70083, array([52833, 33033,  8341, 61107])),\n",
       " (8341, array([33033, 70083, 61107, 11216])),\n",
       " (61107, array([70083,  8341, 11216, 67517])),\n",
       " (11216, array([ 8341, 61107, 67517, 32560])),\n",
       " (67517, array([61107, 11216, 32560, 52327])),\n",
       " (32560, array([11216, 67517, 52327, 35572])),\n",
       " (52327, array([67517, 32560, 35572, 28150])),\n",
       " (35572, array([32560, 52327, 28150, 35811])),\n",
       " (28150, array([52327, 35572, 35811, 54779])),\n",
       " (35811, array([35572, 28150, 54779, 55247])),\n",
       " (54779, array([28150, 35811, 55247, 55823])),\n",
       " (55247, array([35811, 54779, 55823, 53054])),\n",
       " (55823, array([54779, 55247, 53054, 45688])),\n",
       " (53054, array([55247, 55823, 45688, 14352])),\n",
       " (45688, array([55823, 53054, 14352, 72340])),\n",
       " (14352, array([53054, 45688, 72340, 46184])),\n",
       " (72340, array([45688, 14352, 46184, 41898])),\n",
       " (46184, array([14352, 72340, 41898, 52948])),\n",
       " (41898, array([72340, 46184, 52948, 13468])),\n",
       " (52948, array([46184, 41898, 13468, 22816])),\n",
       " (13468, array([41898, 52948, 22816, 10371])),\n",
       " (22816, array([52948, 13468, 10371, 54779])),\n",
       " (10371, array([13468, 22816, 54779, 13931])),\n",
       " (54779, array([22816, 10371, 13931, 79462])),\n",
       " (13931, array([10371, 54779, 79462, 41898])),\n",
       " (79462, array([54779, 13931, 41898, 20817])),\n",
       " (41898, array([13931, 79462, 20817, 78448])),\n",
       " (20817, array([79462, 41898, 78448, 13468])),\n",
       " (78448, array([41898, 20817, 13468, 14074])),\n",
       " (13468, array([20817, 78448, 14074, 10331])),\n",
       " (14074, array([78448, 13468, 10331, 54779])),\n",
       " (10331, array([13468, 14074, 54779, 24786])),\n",
       " (54779, array([14074, 10331, 24786, 10833])),\n",
       " (24786, array([10331, 54779, 10833, 13468])),\n",
       " (10833, array([54779, 24786, 13468, 12834])),\n",
       " (13468, array([24786, 10833, 12834, 26739])),\n",
       " (12834, array([10833, 13468, 26739, 66602])),\n",
       " (26739, array([13468, 12834, 66602, 48261])),\n",
       " (66602, array([12834, 26739, 48261, 30054])),\n",
       " (48261, array([26739, 66602, 30054, 13468])),\n",
       " (30054, array([66602, 48261, 13468,  6310])),\n",
       " (13468, array([48261, 30054,  6310, 13749])),\n",
       " (6310, array([30054, 13468, 13749, 54779])),\n",
       " (13749, array([13468,  6310, 54779, 26931])),\n",
       " (54779, array([ 6310, 13749, 26931, 78448])),\n",
       " (26931, array([13749, 54779, 78448, 13468])),\n",
       " (78448, array([54779, 26931, 13468, 35761])),\n",
       " (13468, array([26931, 78448, 35761, 13468])),\n",
       " (35761, array([78448, 13468, 13468, 42671])),\n",
       " (13468, array([13468, 35761, 42671, 46184])),\n",
       " (42671, array([35761, 13468, 46184, 64612])),\n",
       " (46184, array([13468, 42671, 64612, 56601])),\n",
       " (64612, array([42671, 46184, 56601, 55823])),\n",
       " (56601, array([46184, 64612, 55823, 25734])),\n",
       " (55823, array([64612, 56601, 25734, 46800])),\n",
       " (25734, array([56601, 55823, 46800,  7313])),\n",
       " (46800, array([55823, 25734,  7313, 46382])),\n",
       " (7313, array([25734, 46800, 46382, 80707])),\n",
       " (46382, array([46800,  7313, 80707,  1513])),\n",
       " (80707, array([ 7313, 46382,  1513, 18684])),\n",
       " (1513, array([46382, 80707, 18684, 62633])),\n",
       " (18684, array([80707,  1513, 62633, 44509])),\n",
       " (62633, array([ 1513, 18684, 44509, 46184])),\n",
       " (44509, array([18684, 62633, 46184, 41898])),\n",
       " (46184, array([62633, 44509, 41898, 45769])),\n",
       " (41898, array([44509, 46184, 45769, 80559])),\n",
       " (45769, array([46184, 41898, 80559, 79237])),\n",
       " (80559, array([41898, 45769, 79237, 45502])),\n",
       " (79237, array([45769, 80559, 45502, 19634])),\n",
       " (45502, array([80559, 79237, 19634, 45180])),\n",
       " (19634, array([79237, 45502, 45180, 46184])),\n",
       " (45180, array([45502, 19634, 46184, 48473])),\n",
       " (46184, array([19634, 45180, 48473, 26767])),\n",
       " (48473, array([45180, 46184, 26767, 41898])),\n",
       " (26767, array([46184, 48473, 41898, 39163])),\n",
       " (41898, array([48473, 26767, 39163, 68034])),\n",
       " (39163, array([26767, 41898, 68034, 65412])),\n",
       " (68034, array([41898, 39163, 65412, 70832])),\n",
       " (65412, array([39163, 68034, 70832, 19634])),\n",
       " (70832, array([68034, 65412, 19634, 40506])),\n",
       " (19634, array([65412, 70832, 40506, 58495])),\n",
       " (40506, array([70832, 19634, 58495, 76643])),\n",
       " (58495, array([19634, 40506, 76643,     0])),\n",
       " (76643, array([40506, 58495,     0,     0]))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = padded_corpus[0]\n",
    "contexts = []\n",
    "for context in range(window, len(test) - window):\n",
    "    center_word = test[context]\n",
    "    context_words = np.concatenate((test[(context - window) : context], \n",
    "                                    test[context + 1 : context + window + 1]))\n",
    "    contexts.append((center_word, context_words))\n",
    "print(len(contexts))\n",
    "print(len(indexed_corpus[0]))\n",
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_contexts(document, window):\n",
    "    contexts = []\n",
    "    for context in range(window, len(document) - window):\n",
    "        center_word = document[context]\n",
    "        context_words = np.concatenate((document[(context - window) : context], \n",
    "                                        document[(context + 1) : (context + window + 1)]))\n",
    "        contexts.append((center_word, context_words))\n",
    "    return contexts\n",
    "\n",
    "corpus_document_contexts = [document_contexts(document, window) for document in padded_corpus]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we just care about contexts right now, so flatten the list\n",
    "corpus_contexts = [pair for document_contexts in corpus_document_contexts for pair in document_contexts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(78448, array([    0,     0, 76643,  8341])),\n",
       " (76643, array([    0, 78448,  8341, 61107])),\n",
       " (8341, array([78448, 76643, 61107, 11216]))]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_contexts[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_contexts_tensor = list(map(list, zip(*corpus_contexts)))\n",
    "\n",
    "corpus_center_words = torch.tensor(corpus_contexts_tensor[0], dtype = torch.int64)\n",
    "corpus_context_words = torch.tensor(corpus_contexts_tensor[1], dtype = torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2539447"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_center_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSG(nn.Module):\n",
    "    def __init__(self, vocab_size, input_dim=50, hidden_dim=50, latent_dim=100, margin=1., model_name='BSG with the hinge loss'):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        :param vocab_size: the number of unique words\n",
    "        :param input_dim: the number of components in the encoder's word embeddings\n",
    "        :param hidden_dim: the number of components in the encoder's hidden layer\n",
    "        :param latent_dim: the number of components in the latent vector(also output word mu's)\n",
    "        :param margin: margin constant present in the hinge loss\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.margin = margin\n",
    "\n",
    "        # assign full parameters\n",
    "#         self.params_full = self.__build_model()\n",
    "\n",
    "        # extract only the actual parameter data-structures(tensors) as those will be optimized\n",
    "#         self.params = [param.value for param in self.params_full.values()]\n",
    "\n",
    "        # user accessible functions build ( e.g. training functions)\n",
    "#         self.__build_functions()\n",
    "\n",
    "        # encoder layers\n",
    "        self.encoder_embedding = nn.Embedding(self.vocab_size, self.input_dim, padding_idx = 0) # R\n",
    "        self.encoder_lin1 = nn.Linear(self.input_dim*2, self.hidden_dim) # M\n",
    "        self.encoder_mu = nn.Linear(self.hidden_dim, self.latent_dim) # U -> mu\n",
    "        self.encoder_logsigma = nn.Linear(self.hidden_dim, 1) # W -> log sigma\n",
    "        \n",
    "        # beta\n",
    "        self.type_means = nn.Embedding(self.vocab_size, self.latent_dim)\n",
    "        self.type_vars = nn.Embedding(self.vocab_size, 1)\n",
    "        \n",
    "        \n",
    "    def encoder(self, centers_batch, contexts_batch):\n",
    "#             batch_size = centers_batch.shape[0]\n",
    "        sums = []\n",
    "        for center, context in zip(centers_batch, contexts_batch):\n",
    "            embed_center = self.encoder_embedding(center)\n",
    "            embed_context = self.encoder_embedding(context)\n",
    "            assert embed_context.shape[1] == self.input_dim, \"context embedding is not a 2d tensor\"\n",
    "            center_repeats = embed_center.repeat(2*window, 1)\n",
    "            concat = torch.cat((embed_context, center_repeats),1)\n",
    "            sum_relu_en1 = F.relu(self.encoder_lin1(concat)).sum(0) # a vector\n",
    "            sums.append(sum_relu_en1) # the vectors of sums\n",
    "        sums = torch.stack(sums)\n",
    "        mu = self.encoder_mu(sums)\n",
    "        logsigma = self.encoder_logsigma(sums)\n",
    "        return mu, logsigma\n",
    "    \n",
    "    def reparameterize(self, centers_batch, posterior_mean, posterior_logvar):\n",
    "        eps = Variable(centers_batch.data.new().resize_as_(posterior_mean.data).normal_())\n",
    "        z = posterior_mean + posterior_logvar.exp().sqrt() * eps\n",
    "        return z\n",
    "    \n",
    "    def forward(...):\n",
    "        # we don't need \"generative\"\n",
    "        \n",
    "    def loss(...):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BSG(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,     0, 76643,  8341])\n",
      "tensor([    0, 78448,  8341, 61107])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.4168,  0.0151,  0.0527,  0.1214,  0.2159,  0.3550,  0.9378, -0.4628,\n",
       "           0.3208, -0.9953,  0.8756, -0.0158,  0.7749, -0.8627, -0.0899, -0.1131,\n",
       "          -0.0692,  0.5444, -0.5831,  0.2353,  0.9599, -0.5957, -0.0175,  0.6622,\n",
       "           0.0822, -0.8282, -0.1901,  0.1267,  1.0530, -0.3138, -0.3428, -0.6973,\n",
       "          -0.1746,  0.1953,  0.8736, -0.0369,  0.2305, -0.2232, -0.5545,  0.0326,\n",
       "           0.3026, -0.3336, -0.0501,  0.2016,  0.7545, -0.6623,  0.9983, -0.6342,\n",
       "           0.6753,  0.5179,  0.3885, -0.1390, -0.0223,  0.2696, -0.2688,  0.3179,\n",
       "           0.7157,  0.2432, -0.2337, -0.6568, -0.4430,  0.0980, -0.3330,  0.1981,\n",
       "          -0.1504, -0.3231, -0.0219, -0.0774, -0.0477,  0.3653,  0.0942,  1.1136,\n",
       "           0.3845, -0.2861,  0.4235,  0.7650,  0.5147,  0.4593,  0.2526,  0.3219,\n",
       "           0.5714, -0.9353, -0.2499, -0.1712, -0.4432, -0.1919, -0.4525, -0.0849,\n",
       "          -0.0420,  0.4507, -0.0046,  0.3819,  0.4897,  0.6062,  0.0190, -0.0479,\n",
       "           0.4566, -0.1666,  0.0494, -0.2526],\n",
       "         [ 0.4054,  0.7288, -0.2818,  1.5328, -0.4201,  0.6089,  0.6864,  1.6939,\n",
       "           1.4330,  0.0570,  1.6634, -0.9596,  0.5668, -1.0413, -0.2465,  0.4273,\n",
       "          -0.0367,  1.4930,  0.4974, -0.6166,  1.0198, -1.3427, -0.9977,  0.4587,\n",
       "          -0.3607, -1.0010,  1.2614, -0.8497,  1.3584,  0.4293,  0.3610,  0.0981,\n",
       "          -1.9525, -0.1737,  1.3567,  0.1792, -0.3602,  1.1076, -0.8021, -0.3428,\n",
       "          -0.3356, -0.4778,  0.2349, -0.6904,  0.4095, -1.0645,  0.3767, -0.2445,\n",
       "           1.1729,  0.4370,  0.1005,  0.3270, -0.0983,  0.8664,  1.1670, -0.5295,\n",
       "           0.2501, -0.2307, -1.1211, -0.7669, -0.0877, -0.9931,  1.1045,  1.2949,\n",
       "          -0.3738, -0.2473, -0.3345, -0.7326,  0.7090,  0.2354, -0.9809, -0.1772,\n",
       "           0.5899, -0.3404,  0.5023, -0.4138,  0.8560,  0.1114,  0.4385, -0.9981,\n",
       "           0.5509, -0.9464,  0.3237, -0.0351, -0.7345, -0.9423, -1.5981, -0.8074,\n",
       "           0.7321, -0.4110, -0.4034,  0.8403,  0.4576,  0.3383,  0.5585, -0.7327,\n",
       "           0.7465,  0.1422, -0.0228, -0.0842]], grad_fn=<ThAddmmBackward>),\n",
       " tensor([[0.6395],\n",
       "         [0.0313]], grad_fn=<ThAddmmBackward>))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder(corpus_center_words[0:2], corpus_context_words[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "BSGcollab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
