{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kristynpantoja/bsg_pytorch/blob/master/BSGcollab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1chxO5BA_thF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# import gensim.downloader as api\n",
    "# from gensim.models import Word2Vec, FastText, KeyedVectors\n",
    "# from os.path import isfile\n",
    "\n",
    "import random\n",
    "random.seed(1234)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments \n",
    "\n",
    "\n",
    "\n",
    "window = 2\n",
    "# C = 2*window\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5Ze_s59Ckvr"
   },
   "outputs": [],
   "source": [
    "# Get corpus: 20 news groups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "# tokenize and preprocess\n",
    "\n",
    "tokenizer = RegexpTokenizer(u'(?ui)\\\\b[a-z]{3,}\\\\b')\n",
    "\n",
    "tokenized_corpus = [tokenizer.tokenize(document.lower()) for document in newsgroups_train.data]\n",
    "\n",
    "# normalize_corpus = [normalize(document) for document in tokenized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer) == nltk.tokenize.regexp.RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YeZLkgV-Bz2j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81072"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get vocabulary\n",
    "\n",
    "flattened = [word for document in tokenized_corpus for word in document]\n",
    "vocabulary = set(flattened)\n",
    "vocabulary = list(vocabulary)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JzgbYAuOFavG"
   },
   "outputs": [],
   "source": [
    "# # get vocabulary: with some preprocessing\n",
    "# vectorizer = CountVectorizer(stop_words = 'english', \n",
    "#                              min_df=.01, max_df=0.9, \n",
    "#                              token_pattern = u'(?ui)\\\\b[a-z]{3,}\\\\b')\n",
    "\n",
    "# count_vecs = vectorizer.fit_transform(newsgroups_train.data)\n",
    "\n",
    "# # doc_term_matrix = count_vecs.toarray()\n",
    "# # doc_term_matrix.shape # number of documents, number of words (in vocab)\n",
    "\n",
    "# # note: vectorizer.get_feature_names() != vectorizer.vocabulary_\n",
    "\n",
    "# vocabulary = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "m86kafoIFxB1",
    "outputId": "159aad0c-7c05-43c0-b4aa-3448697475da"
   },
   "outputs": [],
   "source": [
    "# # vocabulary = []\n",
    "# # for sentence in tokenized_corpus:\n",
    "# #     for token in sentence:\n",
    "# #         if token not in vocabulary:\n",
    "# #             vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx + 1 for (idx , w) in enumerate(vocabulary)}\n",
    "idx2word = {idx + 1: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "# idx_pairs = []\n",
    "# # for each sentence\n",
    "# for sentence in tokenized_corpus:\n",
    "#     indices = [word2idx[word] for word in sentence]\n",
    "#     # for each word, threated as center word\n",
    "#     for center_word_pos in range(len(indices)):\n",
    "#         # for each window position\n",
    "#         for w in range(-window_size, window_size + 1):\n",
    "#             context_word_pos = center_word_pos + w\n",
    "#             # make soure not jump out sentence\n",
    "#             if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "#                 continue\n",
    "#             context_word_idx = indices[context_word_pos]\n",
    "#             idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "# idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "v-cZqtyn9mj6",
    "outputId": "d94a233f-9dbd-428f-fd04-252c4a3126ca"
   },
   "outputs": [],
   "source": [
    "indexed_corpus = []\n",
    "# for each document\n",
    "for document in tokenized_corpus:\n",
    "    ragged_array = []\n",
    "    # for each word\n",
    "    for word in document:\n",
    "        ragged_array.append(word2idx[word])\n",
    "    indexed_corpus.append(ragged_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "y1KzDkliJDmg",
    "outputId": "fb72dd79-a0c8-48ff-f79a-6b3ba3792722"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'lerxst', 'wam', 'umd', 'edu', 'where', 'thing', 'subject', 'what', 'car', 'this', 'nntp', 'posting', 'host', 'wam', 'umd', 'edu', 'organization', 'university', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'anyone', 'out', 'there', 'could', 'enlighten', 'this', 'car', 'saw', 'the', 'other', 'day', 'was', 'door', 'sports', 'car', 'looked', 'from', 'the', 'late', 'early', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'the', 'body', 'this', 'all', 'know', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'where', 'this', 'car', 'made', 'history', 'whatever', 'info', 'you', 'have', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'brought', 'you', 'your', 'neighborhood', 'lerxst']\n",
      "97\n",
      "[39987, 69032, 35576, 40308, 3842, 64733, 65043, 30271, 59499, 79491, 17639, 24749, 76239, 27445, 35576, 40308, 3842, 59105, 18639, 3252, 13802, 36077, 4152, 68934, 74553, 2347, 53060, 14423, 16290, 64720, 17639, 79491, 4056, 48297, 76908, 8023, 68934, 18780, 50447, 79491, 55576, 39987, 48297, 8239, 67273, 68934, 59082, 44786, 48297, 53124, 31614, 60842, 77626, 51123, 48297, 46017, 313, 68934, 60455, 39987, 48297, 40000, 48297, 13953, 17639, 25987, 36690, 2347, 54133, 80352, 32478, 6651, 33717, 56186, 62517, 70811, 64733, 17639, 79491, 59414, 19514, 53236, 35383, 22824, 80369, 17639, 73663, 51091, 79491, 50983, 46383, 72312, 25215, 22824, 13747, 50111, 69032]\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_corpus[0])\n",
    "print(len(tokenized_corpus[0]))\n",
    "print(indexed_corpus[0])\n",
    "print(len(indexed_corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HnwGjRNfJFnn"
   },
   "outputs": [],
   "source": [
    "padded_corpus = [np.pad(document, (window,window), 'constant', constant_values=(0, 0)) \n",
    "                 for document in indexed_corpus]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test = padded_corpus[0]\n",
    "# contexts = []\n",
    "# for context in range(window, len(test) - window):\n",
    "#     center_word = test[context]\n",
    "#     context_words = np.concatenate((test[(context - window) : context], \n",
    "#                                     test[context + 1 : context + window + 1]))\n",
    "#     contexts.append((center_word, context_words))\n",
    "# print(len(contexts))\n",
    "# print(len(indexed_corpus[0]))\n",
    "# contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the contexts for each word by looking in each document\n",
    "\n",
    "def document_contexts(document, window):\n",
    "    contexts = []\n",
    "    for context in range(window, len(document) - window):\n",
    "        center_word = document[context]\n",
    "        context_words = np.concatenate((document[(context - window) : context], \n",
    "                                        document[(context + 1) : (context + window + 1)]))\n",
    "        contexts.append((center_word, context_words))\n",
    "    return contexts\n",
    "\n",
    "corpus_document_contexts = [document_contexts(document, window) for document in padded_corpus]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we just care about contexts right now, so flatten the list\n",
    "corpus_centers_contexts = [pair for document_contexts in corpus_document_contexts for pair in document_contexts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(39987, array([    0,     0, 69032, 35576])),\n",
       " (69032, array([    0, 39987, 35576, 40308])),\n",
       " (35576, array([39987, 69032, 40308,  3842]))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_centers_contexts[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_centers_contexts = list(map(list, zip(*corpus_centers_contexts)))\n",
    "\n",
    "corpus_center_words = torch.tensor(corpus_centers_contexts[0], dtype = torch.int64)\n",
    "corpus_context_words = torch.tensor(corpus_centers_contexts[1], dtype = torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_centers_contexts[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_center_words[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_center_words_for_negative_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_center_words_for_negative_sampling = corpus_centers_contexts[0]\n",
    "corpus_centers, corpus_centers_counts = np.unique(corpus_center_words_for_negative_sampling, \n",
    "                                                  return_counts=True)\n",
    "unigram_dict = dict(zip(corpus_centers, corpus_centers_counts/len(corpus_center_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_centers_contexts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_center_words.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSG(nn.Module):\n",
    "    def __init__(self, unigram_dict, vocab_size, input_dim=50, hidden_dim=50, latent_dim=100, margin=1., model_name='BSG with the hinge loss'):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        :param vocab_size: the number of unique words\n",
    "        :param input_dim: the number of components in the encoder's word embeddings\n",
    "        :param hidden_dim: the number of components in the encoder's hidden layer\n",
    "        :param latent_dim: the number of components in the latent vector(also output word mu's)\n",
    "        :param margin: margin constant present in the hinge loss\n",
    "        \"\"\"\n",
    "        self.window = window\n",
    "        self.model_name = model_name\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.margin = margin\n",
    "        \n",
    "        self.unigram_dist = torch.distributions.Categorical(torch.tensor(list(unigram_dict.values())))\n",
    "\n",
    "        # assign full parameters\n",
    "#         self.params_full = self.__build_model()\n",
    "\n",
    "        # extract only the actual parameter data-structures(tensors) as those will be optimized\n",
    "#         self.params = [param.value for param in self.params_full.values()]\n",
    "\n",
    "        # user accessible functions build ( e.g. training functions)\n",
    "#         self.__build_functions()\n",
    "\n",
    "        # encoder layers\n",
    "        self.encoder_embedding = nn.Embedding(self.vocab_size+1, self.input_dim, padding_idx = 0) # R\n",
    "        self.encoder_lin1 = nn.Linear(self.input_dim*2, self.hidden_dim) # M\n",
    "        self.encoder_mu = nn.Linear(self.hidden_dim, self.latent_dim) # U -> mu\n",
    "        self.encoder_logsigma = nn.Linear(self.hidden_dim, 1) # W -> log sigma\n",
    "        \n",
    "        # word embeddings' parameters for normal distributions of word types\n",
    "        self.type_means = nn.Embedding(self.vocab_size+1, self.latent_dim)\n",
    "        self.type_logvars = nn.Embedding(self.vocab_size+1, 1)\n",
    "        \n",
    "        \n",
    "    def encoder(self, centers_batch, contexts_batch):\n",
    "#             batch_size = centers_batch.shape[0]\n",
    "        b, C = contexts_batch.shape\n",
    "        assert C == 2*self.window, \"C does not equal 2*window\"\n",
    "        embed_centers = self.encoder_embedding(centers_batch)\n",
    "        centers_with_3rd_dim = embed_centers.unsqueeze(1) # batch by 1 by hidden\n",
    "        repr_center = centers_with_3rd_dim.repeat(1, C, 1) # centers as a matrix\n",
    "        repr_context = self.encoder_embedding(contexts_batch)\n",
    "        \n",
    "        repr_common = torch.cat((repr_center, repr_context), 2)\n",
    "        \n",
    "        hidden = F.relu(self.encoder_lin1(repr_common)).sum(1) # ?\n",
    "        mu = self.encoder_mu(hidden)\n",
    "        logsigma = self.encoder_logsigma(hidden)\n",
    "        return mu, logsigma\n",
    "        \n",
    "#         sums = []\n",
    "#         for center, context in zip(centers_batch, contexts_batch):\n",
    "#             embed_center = self.encoder_embedding(center)\n",
    "#             embed_context = self.encoder_embedding(context)\n",
    "#             assert embed_context.shape[1] == self.input_dim, \"context embedding is not a 2d tensor\"\n",
    "#             center_repeats = embed_center.repeat(2*window, 1)\n",
    "#             concat = torch.cat((embed_context, center_repeats),1)\n",
    "#             sum_relu_en1 = F.relu(self.encoder_lin1(concat)).sum(0) # a vector\n",
    "#             sums.append(sum_relu_en1) # the vectors of sums\n",
    "#         sums = torch.stack(sums)\n",
    "#         mu = self.encoder_mu(sums)\n",
    "#         logsigma = self.encoder_logsigma(sums)\n",
    "#         return mu, logsigma\n",
    "    \n",
    "    def reparameterize(self, centers_batch, posterior_mean, posterior_logvar):\n",
    "        eps = Variable(centers_batch.data.new().resize_as_(posterior_mean.data).normal_())\n",
    "        z = posterior_mean + posterior_logvar.exp().sqrt() * eps\n",
    "        return z\n",
    "    \n",
    "    def KL(self, word_idx, post_mu, post_logsigma):\n",
    "        post_sigma = post_logsigma.exp()\n",
    "        type_mean = self.type_means(word_idx)\n",
    "        type_var = self.type_logvars(word_idx).exp().view([-1,post_logsigma.shape[1]])\n",
    "#         print(\"type_var\")\n",
    "#         print(type_var.shape)\n",
    "        var_division = post_sigma / type_var\n",
    "        diff = post_mu - type_mean\n",
    "        diff_term = (diff * diff).sum(1) / type_var ## added the .sum(1) from the original KL in our AVITM implementation of KL\n",
    "        logvar_division = type_var.log() - post_logsigma\n",
    "        # compute KL\n",
    "        KLD = 0.5 * ( (var_division + diff_term + logvar_division).sum(1) - self.latent_dim )\n",
    "        return KLD\n",
    "        \n",
    "    def forward(self, centers_batch, contexts_batch):\n",
    "#         print(\"contexts_batch.shape - should be batch size by context window size\")\n",
    "#         print(contexts_batch.shape)\n",
    "        mu, logsigma = model.encoder(centers_batch, contexts_batch)\n",
    "        # repeat mu, logsigma 2*window times, \n",
    "        #  one for each context word in an input\n",
    "        #  - do this for each input in the batch\n",
    "        mus = mu.repeat(1,window*2).view(-1,mu.shape[1]) \n",
    "        logsigmas = logsigma.repeat(1,window*2).view(-1,logsigma.shape[1])\n",
    "        # compute KLs\n",
    "        KL_contexts = self.KL(contexts_batch.view([-1,1]), mus, logsigmas)\n",
    "        negative_contexts_batch = self.unigram_dist.sample(contexts_batch.shape) + 1\n",
    "        KL_negative_contexts = self.KL(negative_contexts_batch.view([-1,1]), mus, logsigmas)\n",
    "        KL_center_word = self.KL(centers_batch, mu, logsigma)\n",
    "        \n",
    "        # compute hard margin of KLs of negative and positive context words\n",
    "        hard_margin_arg = KL_contexts - KL_negative_contexts + self.margin\n",
    "        loss = torch.max(hard_margin_arg, torch.zeros_like(hard_margin_arg)).sum() + KL_center_word.sum()\n",
    "        return loss / centers_batch.shape[0]\n",
    "    \n",
    "# b, window_size = pos_context_words.shape\n",
    "#         sigma_q = T.repeat(sigma_q, window_size, axis=0)\n",
    "#         mu_q = T.repeat(mu_q, window_size, axis=0)\n",
    "\n",
    "#         pos_c_resh = pos_context_words.reshape((-1, ))\n",
    "#         mu_p_pos, sigma_p_pos = self.__compute_prior_params(pos_c_resh)\n",
    "\n",
    "#         neg_c_resh = neg_context_words.reshape((-1, ))\n",
    "#         mu_p_neg, sigma_p_neg = self.__compute_prior_params(neg_c_resh)\n",
    "\n",
    "#         kl_pos = self.kl(mu_q, sigma_q, mu_p_pos, sigma_p_pos).reshape((b, -1))\n",
    "#         kl_neg = self.kl(mu_q, sigma_q, mu_p_neg, sigma_p_neg).reshape((b, -1))\n",
    "\n",
    "#         # hard margin\n",
    "#         return T.sum(T.maximum(0.0, self.margin - kl_neg + kl_pos) * mask, axis=1)\n",
    "    \n",
    "def train(model, args, optimizer, center_words, context_words):\n",
    "    '''\n",
    "    model - object of class BSG\n",
    "    args - dict of args\n",
    "    optimizer - nn.optim\n",
    "    centers_batch, contexts_batch\n",
    "    '''\n",
    "    for epoch in range(args.num_epoch):\n",
    "        all_indices = torch.randperm(context_words.size(0)).split(args.batch_size)\n",
    "        loss_epoch = 0.0\n",
    "        model.train()                   # switch to training mode\n",
    "        for batch_indices in all_indices:\n",
    "            if not args.nogpu: batch_indices = batch_indices.cuda()\n",
    "            context_words_input = Variable(context_words[batch_indices])\n",
    "            center_words_input = Variable(center_words[batch_indices])\n",
    "            loss = model(center_words_input, context_words_input)\n",
    "            # optimize\n",
    "            optimizer.zero_grad()       # clear previous gradients\n",
    "            loss.backward()             # backprop\n",
    "            optimizer.step()            # update parameters\n",
    "            # report\n",
    "            loss_epoch += loss.data[0]    # add loss to loss_epoch\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch {}, loss={}'.format(epoch, loss_epoch / len(all_indices)))\n",
    "#         model.losses.append(loss_epoch / len(all_indices))\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments for BSG model\n",
    "\n",
    "args_dict = {\"vocab_size\" : vocabulary_size, \"window\" : window, \n",
    "             \"batch_size\" : 200, \"optimizer\" : 80, \"learning_rate\" : 0.0005, \n",
    "             \"momentum\" : 0.99, \"num_epoch\" : 50, \"init_mult\" : 1, \n",
    "             \"variance\" : 0.995, \"start\" : True, \"nogpu\" : True, \n",
    "             \"embedding_dim\" : 300, \"freeze\" : False}\n",
    "\n",
    "args = SimpleNamespace(**args_dict)\n",
    "# args.num_input = doc_term_matrix_tensor.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BSG(unigram_dict, args.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, logsigma = model.encoder(corpus_center_words[0:2], corpus_context_words[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.forward(corpus_center_words[0:2], corpus_context_words[0:2]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_center_words[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KLs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### repeat mus in the right order\n",
    "\n",
    "# mus = []\n",
    "# for row in mu:\n",
    "#     mus.append([row.repeat(window, 1)])\n",
    "# flattened = [word for document in mus for word in document]\n",
    "# torch.cat(flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_context_words[0:2].view([-1,1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_context_words[0:2].reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.repeat(1,window*2).view(-1,mu.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), args.learning_rate, betas=(args.momentum, 0.999))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/math689env/lib/python3.7/site-packages/ipykernel/__main__.py:147: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss=271148.875\n",
      "Epoch 5, loss=182078.59375\n",
      "Epoch 10, loss=161547.96875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-fa5c6adc007a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_center_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_context_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-d829598b2ba7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, args, optimizer, center_words, context_words)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# clear previous gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;31m# update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;31m# report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/math689env/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/math689env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train(model, args, optimizer, corpus_center_words[0:1000], corpus_context_words[0:1000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "BSGcollab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
